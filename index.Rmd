---
title: "mlm-workshop"
author: "Philip"
date: "1 december 2018"
output: 
 html_document:
  toc: true
  toc_depth: 3
  toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 0, digits = 2)
```

#Load data and packages

```{r, message=F}
library(brms)
library(ggplot2)
library(bayesplot)
library(tidyverse)
library(kableExtra)
library(brmstools)
library(sjstats)
```

```{r}
#set up number of cores to use for fitting
n_cores <- parallel::detectCores()-1
```


```{r load data}
d <- read.delim(url("https://raw.githubusercontent.com/philipparnamets/mlm-workshop/master/data/d_export.txt"))
```

```{r}
summary(d)
```

Data comes from a trust learning experiment and represents a subset of 30 participants. Each participants did 30 trials with social partners from a trustworthy group and 30 trials with social partners from an untrustworthy group. 

The aim of the analysis will be to understand if participant **response times differ between groups**.


#Data visualization and preparation

##Relationship between RT and Trustworthiness

```{r}
# create factor
d$Trustworthy_fac <- factor(d$Trustworthy, labels = c("Untrustworthy",
                                                      "Trustworthy"))

# summary table
d %>% group_by(Trustworthy_fac) %>%
  summarise(Average = mean(RT),
            Median = median(RT),
            SD = sd(RT),
            SE = SD/sqrt(n())) %>%
  kable(.) %>%
  kable_styling(., bootstrap_options = c("striped", "condensed"),
                full_width = F)
```

```{r}
# plot
ggplot(data = d) +
  aes(x = Trustworthy_fac, y = RT, 
      fill = Trustworthy_fac) +
  geom_violin() +
  geom_boxplot(fill="gray", 
               notch = T ,
               width = 0.4) 

```


Lot's of outliers, let's filter the data at 10s, which seems to be plenty to make a response anyways.. 

```{r}
# plot
d %>% filter(RT<10) %>%
ggplot(data = .) +
  aes(x = Trustworthy_fac, y = RT, 
      fill = Trustworthy_fac) +
  geom_violin() +
  geom_boxplot(fill="gray", 
               notch = T ,
               width = 0.4) 

```

Response time data are skewed:

```{r}
ggplot(d) +
  aes(RT) +
  geom_histogram(binwidth = 0.25)
```

One way of dealing with this is log-transforming the data.

```{r}
# log transform
d$RT_log <- log(d$RT)

#plot 
ggplot(d) +
  aes(RT_log) +
  geom_histogram(binwidth = 0.1)
```


##Face trustworthiness

The variable *Face_trust* captures how trustworthy the face of the partner has been rated to be. 

```{r}
ggplot(d) +
  aes(Face_trust) +
  geom_histogram(binwidth = 0.1)
```

```{r}
# scale variable
d$Face_trust_s <- scale(d$Face_trust)
```


#Setting up the model

There are four important components to fitting a model in *brms*:

- Data
- Likelihood
- Model formula
- Prior

We have already set up our data.

##Likelihood

SAY SOMETHING MORE ABOUT THE LIKELIHOOD HERE

##Model formula

*brms* uses the same syntax as *lme4*, which simplifies the transition. 

```{r}

ff <- 
  bf(RT ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli))

```

We want to estimate the effect of Trustworthiness on response times, while taking into account the facial features of the social partners and their interaction wtih Trustworthiness. Additionally, we want to allow all slopes and intercept **vary** within participants and stimuli. 

##Priors

The arguably most tricky bit when it comes to getting started with Bayesian modelling is setting and understanding priors. First we see what priors the model requires: 

```{r}
kable(get_prior(formula = ff, family = gaussian(),
          data = d)) %>%
  kable_styling(c("striped", "condensed"), full_width = F)
```

We begin by setting some weakly informative priors.

```{r}
pp <- c(set_prior("normal(0,1)", class = "b"), #this is for the population average slopes
        set_prior("normal(0,10)", class = "sd", group = "Subject"),
        set_prior("normal(0,5)", class = "sd", group = "Stimuli"),
        set_prior("normal(0,5)", class = "sigma"),
        set_prior("lkj(3)", class = "cor", group = "Stimuli"),
        set_prior("lkj(3)", class = "cor", group = "Subject"))
```


```{r, results='hide', cache=T}
fit_rt <- brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = "only")
```

```{r}
rt1_prior <- marginal_effects(fit_rt, re_formula = NULL,
                              method = "predict")
plot(rt1_prior, ask = F)
```

Let's make some changes.

```{r}
pp <- c(set_prior("normal(2,5)", class = "Intercept"),
        set_prior("normal(0,1)", class = "b"), #this is for the population average slopes
        set_prior("normal(0,1)", class = "sd", group = "Subject"),
        set_prior("normal(0,1)", class = "sd", group = "Stimuli"),
        set_prior("normal(0,5)", class = "sigma"),
        set_prior("lkj(3)", class = "cor", group = "Stimuli"),
        set_prior("lkj(3)", class = "cor", group = "Subject"))
```


```{r, results='hide', cache=T}
fit_rt2 <- brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = "only")
```


```{r}
rt2_prior <- marginal_effects(fit_rt2, re_formula = NULL,
                              method = "predict")
plot(rt2_prior, ask = F)
```

The model's priors now covers a much more reasonable range of values.

#Fitting the model

Having gone through these steps, sampling from the model is straightforward:

```{r, results='hide', cache=T}
mod_rt2 <-  brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = "yes",
              control = list(adapt_delta = 0.8))
```

##Model diagnostics

Stan (and brms) will warn you if the model is having some particular problems converging. In particular it will tell you if there are *divergent transitions*, which are caused by the model not being able to explore the full parameter space. This can be tricky to fix, but models with divergent transitions should not be trusted. Changing the priors, and changing some of the sampler's internal parameters can help - increasing adapt_delta over it's default (0.8). 

Other diagnostic quantities to pay attention to are the number of effective samples and $\hat{R}$ values.

Both are accessed from the model summary: 

```{r}
summary(mod_rt2)
```

But can also be visualized using the Bayesplot package.

For $\hat{R}$ these should be approximately 1, and definitely no larger than 1.1.

```{r}
all_hats <- rhat(mod_rt2)
color_scheme_set("red")
mcmc_rhat_hist(all_hats)
```

Effective sample size is a measure of how well the model is approximating the posterior distribution of interest. If there are very few effective samples, the resulting posterior is less reliable. This might be due to autocorrelation in the samples. Typically you can sample your model for longer to get more effective samples out of it. 

Here we see that some parameters have a lower ratio of effective samples than others, but none are lower than 0.1, which is a *heuristic* value for when we should really worry:

```{r}
neffs <- neff_ratio(mod_rt2)
mcmc_neff_hist(neffs)
```

```{r}
#here we can see which parameters were affected
mcmc_neff(neffs[neffs<0.5]) + yaxis_text(hjust = 1)
```

If we had been worried about this, we might have wanted to plot the autocorrelation of, say, the intercept. We compare it with another parameter to see the difference: 

```{r}
post <- posterior_samples(mod_rt2, c("b_Intercept", "b_Trustworthy"))
mcmc_acf(post[,1:2])
```


**>>>If we had not been happy with the model diagnostics, we would refit the model before proceeding<<<**

#Working with the posterior

The posterior samples reflect the outcome of the model fitting. We use this to further refine our understanding of the model and to hopefully learn something about the data that we are modelling. 

##Posterior predictive checks

The first thing is to see if the model is capturing relevant features of the data - in other words, if it plausibly is capturing the *underlying data generating process*. 

```{r}
pp_check(mod_rt2)
```

Unurprisingly, our gaussian model cannot capture the full distribution of the data. (But we kind of knew that already, so let's ignore it for now).

Another way of seeing this is how the minimum and maximum predictions differ from those in the data:

```{r}
#make a matrix of posterior predictions
y_pred <- predict(mod_rt2, 
                  nsamples = 500, #500 samples per row of the original data frame
                  summary = F) #if =TRUE, then we will get the mean and 95% CIs instead
dim(y_pred)
```

```{r}
#minimum value
ppc_stat(y = d$RT, yrep = y_pred,
         stat = "min",
         binwidth = 0.05)
```

This reinforces what we knew from the density overlay above.

```{r}
#maximum value
ppc_stat(y = d$RT, yrep = y_pred,
         stat = "max",
         binwidth = 0.05)
```

This reinforces what we know from the prior-predictive checks, the model will not predict extremely large values. In this case, we would probably consider the response time of 30s to be an outlier, and not be concerned about this at all..


We can also make posterior checks dependent on leves of a grouping variable. For example how good is the model at capturing the mean?

```{r}
ppc_stat_grouped(y = d$RT, 
                 yrep = y_pred, 
                 group = d$Trustworthy_fac,
                 binwidth = 0.025)
```

Just like when we looked at the prior predictive checks, we can get the marginal predictions:

```{r}
plot(marginal_effects(mod_rt2), ask = F)
```

You could also make your own plot by using the *y_pred* values, since these are really the same thing. Here we show the full posterior distribution as a boxplot, the average RT in the data (black) and the average predictions (red dot).

```{r}
d$mu_pred <- apply(y_pred,2,mean)

ggplot(d) + 
  aes(x=Trustworthy_fac, y = mu_pred) + 
  geom_boxplot(notch = T) +
  stat_summary(mapping = aes(y=RT, x = Trustworthy_fac), 
               fun.data = "mean_se", geom = "linerange", size = 2) + 
  stat_summary(fun.data = "mean_se", colour = "red")

```


##Summarizing the model

We have already seen the full model summary. Let's make some visualizations as well. 

To do so we will extract samples from the model.

```{r}
post <- posterior_samples(mod_rt2, "^b") 
# ^b is regular expression to select all the population level parameters (beginning with "b_"))
```

We can plot the coefficients:

```{r}
mcmc_intervals(post)
```

We are not particularly interested in the intercept, given how the model was set up, so lets omit it:

```{r}
mcmc_areas(post[,-1],
           prob = 0.8,
           prob_outer = 0.95)
```

Having fit a multi-level model we have access to lots of other parameters that we wouldn't have had otherwise. Let's a quick example. First, we extract the correlation estimates for the Subject level parameters. 

```{r}
post <- posterior_samples(mod_rt2, "^cor_Subject") #correlation parameters
mcmc_areas(post,
           prob = 0.8,
           prob_outer = 0.95)
```

We see that a fairly large negative correlation between subjects' interecept and slopes of Trustworthiness. This would indicate that participants that are slower overall, also differentiate less between conditions in the speed of their responses. To see this, we can look at the estimates for the actual slopes for each participant.


```{r}
post <- posterior_samples(mod_rt2, "^r_Subject") # all Subject level parameters
dim(post)  #120 parameters, 30 subject * 4 estimates
```

Here we plot the average slope, with lines indicating the uncertainty in the estimate (50% level)

```{r}
mu_slopes <- apply(post, 2, mean)
mu_uncert <- apply(post, 2, function(x) quantile(x, c(0.25,.75)))

plot(x = mu_slopes[1:30], y = mu_slopes[31:60],
     xlab = "Intercept", ylab = "Effect of Trustworthiness",
     pch = ".", cex = 5)
abline(h=0, lty = 2)
grid(col = "gray")
segments(x0 = mu_slopes[1:30],
         y0 = mu_uncert[1, 31:60],
         y1 = mu_uncert[2, 31:60],
         lwd = 1, col = "darkgray")
segments(y0 = mu_slopes[31:60],
         x0 = mu_uncert[1, 1:30],
         x1 = mu_uncert[2, 1:30],
         lwd = 1, col = "darkgray")
```

Group varying estimates are deviations from the average, population level estimate. So to understand this plot in terms of the actual effect implied we need to add the population level.

```{r}
mu_slopes <- apply(post, 2, mean)
mu_uncert <- apply(post, 2, function(x) quantile(x, c(0.25,.75)))
trust_av <- fixef(mod_rt2)[2,1]
inter_av <- fixef(mod_rt2)[1,1]

plot(x = mu_slopes[1:30]+inter_av, 
     y = mu_slopes[31:60]+trust_av,
     xlab = "Intercept", ylab = "Effect of Trustworthiness",
     pch = ".", cex = 5)
abline(h=0, lty = 2)
grid(col = "gray")
segments(x0 = mu_slopes[1:30]+inter_av,
         y0 = mu_uncert[1, 31:60]+trust_av,
         y1 = mu_uncert[2, 31:60]+trust_av,
         lwd = 1, col = "darkgray")
segments(y0 = mu_slopes[31:60]+trust_av,
         x0 = mu_uncert[1, 1:30]+inter_av,
         x1 = mu_uncert[2, 1:30]+inter_av,
         lwd = 1, col = "darkgray")
```


From this plot we also see that for example two participants have a positive effect of Trustworthiness. We can plot the group-level average slopes together with population level slope to get a sense for this variation in relation to the average effect. 
(note that this tool would be particularly useful if you are doing a "random effects" meta analysis)

```{r}
forest(mod_rt2, grouping = "Subject", pars = "Trustworthy")
```


##Inference

What conclusions can we draw from the model regarding the parameter estimates and the effect of Trustworthiness on response times?

###Posterior probabilities

The most straightforward method is to assess the posterior distribution of the parameter of interest, in this case, the coefficient of trustworthiness. 

For a typical two-sided inference, we can consider the posterior credible interval, for example the 97% interval, and see if this doesn't include 0:

```{r}
q_97 <- quantile(post$b_Trustworthy, c(0.015, 0.985))
q_97
```

We can also calculate the posterior probability that the effect is less than 0:

```{r}
prob <- sum(post$b_Trustworthy<0)/length(post$b_Trustworthy)
paste(prob*100,"% of samples are less than 0")
```

We can straightforwardly combine this information in a plot:

```{r}
post <- posterior_samples(mod_rt2, "^b")
mcmc_hist(post, "b_Trustworthy", binwidth = 0.025) +
  geom_vline(xintercept = 0, size = 1.5, linetype = "dashed") +
  geom_segment(aes(x = q_97[1], xend = q_97[2], y = 5, yend = 5),
               size = 1.5, colour = "blue") +
  annotate("text", 
           x = mean(q_97), y = 20,
           label = paste("97% CI: [", round(q_97[1],2), "," , round(q_97[2],2), "]" ),
           colour = "blue", fontface = 2) +
  annotate("text",
           x = -0.9, y = 300,
           label = paste(prob*100,"% of samples \nare less than 0"),
           fontface = 2)
```

###HDI & ROPE

An alternative to quantile-based credible intervals is the HDI (Highest posterior Density Interval).  The HDI is constructed to be the narrowest range containg X% of the posterior samples. In many cases these will be the same (symmetric posteriors centered on the mode/median). John Kruschke has written a lot about the HDI & ROPE approach.

To get HDI intervals:

```{r}
hdi(mod_rt2, prob= c(.89, .97))
```

The sjstats package also allows us to get a summary with HDIs:

```{r}
tidy_stan(mod_rt2)
```

We can use the HDIs just as the credible intervals in the previous section. But they come into their own when combined with ROPE-testing.  (ROPE = Region of Practical Equivalence).  The ROPE defines the ranges of parameter values that we consider to be equivalent to a null finding. We then compare the HDI to the ROPE to make a decision about the parameter. If the HDI excludes the parameter entirely, we reject the ROPE region. If the HDI is entirely within the ROPE, we accept the ROPE values. And otherwise we are undecidided.  (**BUT** We can still make probabilistic statements about our parameter!)

Defining a ROPE is tricky, and will depend on knowledge about the effect of interest and the underlying theory. Here pre-registration can play a strong role. 

For our example, let's say we consider effects smaller than 75ms as meaningless and equivalent to 0. (Of course, there is nothing that says that you have to have the same ROPEs for all parameters, but if you scaled the variables you likely will..)

This tells us how much of the posterior is inside and outside the ROPE region. 

```{r}
rope(mod_rt2, rope = c(-0.075,0.075))
```

To get the full equivalence test:

```{r}
equi_test(mod_rt2, rope = c(-.075,.075))
```

Currently, these functions use 95% HDIs, but this hopefully will change in a future release. 

###Bayes Factors

Another method of inference is via Bayes Factors which quanitify the relative evidence in favor or against the null hypothesis. While it is possible to calculate Bayes Factors for entire models (see the *bridgesampling* (package)[https://cran.r-project.org/web/packages/bridgesampling/index.html]), we will look at a simpler method at the parameter level. This is the Savage-Dickey ratio, which simply is the ratio between the prior density and the posterior density at the test value for your hypothesis (0 for a typical null hypothesis).

```{r}
h <- hypothesis(mod_rt2, 
                c("Trustworthy=0",
                  "Face_trust_s=0",
                  "Trustworthy:Face_trust_s=0"))
kable(h$hypothesis) %>%
  kable_styling(c("condensed", "striped"), full_width = F)
```

By default, the Bayes Factor (called Evid.Ratio here) is reported as evidence *for* the null, the $BF_{01}$. If we want the evidence against the null, $BF_{10}$, we simply take the reciprocal.

```{r}
bf_10 <- 1/h$hypothesis$Evid.Ratio
h_new <- cbind(h$hypothesis, bf_10)
names(h_new) <- c(names(h$hypothesis), "BF_10")
kable(h_new) %>%
  kable_styling(c("condensed", "striped"), full_width = F)
```

We can also visualize the hypothesis test: 

```{r}
plot(h)
```

**NOTE**, Bayes Factors are *very* sensitive to the priors, and it is easy to BF-hack 'marginal' results. Pre-registration is useful here as well.


###Model comparison

In some cases we are not interested in making direct inferences about the parameter values but are interested in competing models. This is particularly the case if we are worried about overfitting - since more parameters will always improvde the fit of the model. 

```{r, results='hide', cache=T}
mod_rt3 <- brm(RT ~ 1 + Trustworthy + Face_trust_s  + 
     (1 + Trustworthy + Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s | Stimuli),
     prior = pp, family = gaussian(), data = d,
     iter = 2e3, warmup = 1e3,
     chains = 6, cores = n_cores)
```

The current gold standard for this kind of model comparison is approximate leave-one-out cross validation (LOO). Note that this can be computationally taxing. An alternative, also supported by brms, is WAIC - (Widely Applicable Information Criterion).

```{r, warning = F, message = F, cache=T}
#only run if you have time to wait a bit..
loo2 <- loo(mod_rt2, cores = n_cores, reloo = T)
loo3 <- loo(mod_rt3, cores = n_cores, reloo = T)
```

(For the purpose of this workshop we ignore the warning, but we really shouldn't be..)

```{r}
compare_ic(loo2, loo3)
```

There is no robust difference in predictive performance between the models. 

There is a lot more to be said about LOO, and there is [great online documentation for it](http://mc-stan.org/loo/articles/index.html).

#Refitting the model

This is not a full analysis, but just a brief demonstration of how it could have looked if we had fitted the model to the log-transformed response times instead.


```{r, results='hide', cache=T}
ff <- 
  bf(RT_log ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli))


pp <- c(set_prior("normal(0.4, .2)", class = "Intercept"),
        set_prior("normal(0,.2)", class = "b"), #this is for the population average slopes
        set_prior("normal(0,.2)", class = "sd", group = "Subject"),
        set_prior("normal(0,.2)", class = "sd", group = "Stimuli"),
        set_prior("normal(0,.5)", class = "sigma"),
        set_prior("lkj(3)", class = "cor", group = "Stimuli"),
        set_prior("lkj(3)", class = "cor", group = "Subject"))


mod_rt_log <- brm(formula = ff, data = d,
                  family = gaussian(), prior = pp,
                  iter = 2e3, warmup = 1e3, 
                  chains = 6, cores = n_cores)
```



```{r}
pp_check(mod_rt_log)
```


```{r}
#make a matrix of posterior predictions
y_pred <- predict(mod_rt_log, 
                  nsamples = 500, #500 samples per row of the original data frame
                  summary = F) #if =TRUE, then we will get the mean and 95% CIs instead
```

```{r}
#minimum value
ppc_stat(y = d$RT_log, yrep = y_pred,
         stat = "min",
         binwidth = 0.025)
```


```{r}
#maximum value
ppc_stat(y = d$RT_log, yrep = y_pred,
         stat = "max",
         binwidth = 0.025)
```

```{r}
ppc_stat_grouped(y = d$RT_log, 
                 yrep = y_pred, 
                 group = d$Trustworthy_fac,
                 binwidth = 0.005)
```


We can see the the posterior predidictions are better than before. The downside is that now all our predictors are on a log scale:

```{r}
post <- posterior_samples(mod_rt_log, "^b")
mcmc_areas(post,
           prob = 0.8,
           prob_outer = 0.95)

```


Alternatively we would have changed the likelihood used in the model, using a distribution known to be very good with human response times like the ex-Gaussian (supported by *brms*). See [Heathcote, Popiel & Mewhort, 1991](http://www.tascl.org/uploads/4/9/3/3/49339445/4_.pdf) .
