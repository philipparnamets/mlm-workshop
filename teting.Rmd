---
title: "Advanced example"
output: 
 html_document:
  toc: true
  toc_depth: 2
  toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 0, digits = 2)
```

This example invovles the use of ordinal models to fit Likert scale data, and additionally some of the trickier aspects of choosing a multilevel structure for ones data. 

#Load data and packages

The data come from the first study reportd in a [recent paper](http://ubc-emotionlab.ca/wp-content/files_mf/tracystecklerheltzelinpressjpsp78.pdf) investigating the role of disgust for moral judgments. Participants were divided into two conditions and either given ginger or placebo to eat. Roughly an hour later participants saw pictures depicting disgusting scenes and rated their disgust (1-7). 

```{r, message=F}
library(brms)
library(bayesplot)
library(kableExtra)
library(brmstools)
```

```{r}
#set up number of cores to use for fitting
n_cores <- parallel::detectCores()-1
```

```{r}
dat <- read.delim(url("https://raw.githubusercontent.com/philipparnamets/mlm-workshop/master/data/ginger_data.txt"))
```

```{r}
summary(dat)
```


#Data visualization

```{r}
#define a nice bar plot function
plot_prop <- function(dat, header = "main",
                        y_max = 0.5){
  #bar plot
  par(mar = c(4,5,3,3)+.1)
  plot(NA, main = "", xlab = "", ylab ="",
       ylim = c(0,y_max), xlim = c(0.35,7.65),
       axes = F)
  axis(1, at = 1:7, 
       lwd = 2, cex.axis = 1.1)
  axis(2, las = 2, lwd = 2, cex.axis = 1.1)
  mtext("rating", 1, 2.5, font= 2, cex = 1.2)
  mtext("proportion", 2, 3.5, font= 2, cex = 1.2)
  mtext(header, 3, 1, font= 2, cex = 1.3)
  grid(col = "darkgrey")
  
  tab <- with(dat,
       xtabs(~ rating + Condition))
  tab <- prop.table(tab,2)
  
  for(i in 1:nrow(tab)){
    rect(xleft = i-.4,
         ybottom = 0, 
         xright = i,
         ytop = tab[i,1],
         lwd = 2, col = "darkgray")
    
    rect(xleft = i,
         ybottom = 0, 
         xright = i+.4,
         ytop = tab[i,2],
         lwd = 2, col = "lightgoldenrod")
  }
  
  legend(x = 2, y = y_max,
         legend = c("control", "ginger"),
         fill = c("darkgray", "lightgoldenrod"),
         bty = 'n', cex = 1.2)
}

```

```{r}
plot_prop(dat, "all data")
```


```{r}
#neutral, high, medium items separately
for (cond in unique(dat$valence)){
  d_tmp <- subset(dat, valence == cond)
  plot_prop(dat = d_tmp, header = cond,
            y_max = 1)
}

```

```{r}
violinCustom <- function (data, xpos, scaling = 1,
                          shade = "gray", na_rm = T,
                          whisk = .15){
  #library(sm, quietly = T, verbose=F)
  limit <- c(min(data, na.rm = na_rm),max(data, na.rm = na_rm))
  smout <- sm::sm.density(data, display = "none", xlim = limit)
  
  # draws the poly
  polygon(x = c(smout$estimate*scaling + xpos, xpos - rev(smout$estimate)*scaling) , 
          y = c(smout$eval.points, rev(smout$eval.points)),
          col = shade)
  
  # put a box in it
  half_width <- max(smout$estimate*scaling)/3 # no wider than half maximum
  Q1 <- quantile(data, 0.25, na.rm = na_rm)
  Q3 <- quantile(data, 0.75, na.rm = na_rm)
  Q2 <- median(data, na.rm = na_rm)
  IQR <- Q3-Q1
  upper = Q2 + IQR*1.5
  if (upper > limit[2]) {upper <- limit[2]}
  lower = Q2 - IQR*1.5
  if (lower < limit[1]) {lower <- limit[1]}
  segments(x0 = xpos, x1 = xpos, y0 = lower, y1 = upper, col = "black", lwd =2)
  segments(x0 = xpos-whisk, x1 = xpos+whisk, y0 = lower, y1 = lower, col = "black", lwd = 1.5)
  segments(x0 = xpos-whisk, x1 = xpos+whisk, y0 = upper, y1 = upper, col = "black", lwd = 1.5)
  polygon(x = c(xpos-half_width, xpos+half_width, xpos+half_width, xpos-half_width),
          y = c(Q1,Q1,Q3,Q3), lwd = 2,
          col = rgb(0.9,0.9,0.9, alpha = 0.5))
  points(xpos, Q2, lwd = 3, cex = 1)
  
}

plot_means <- function(dat, header = "main",
                       viol_scale = 0.35){
  par(mar=c(4,5,3,3)+.1)
  plot(NA, ylab = "", xlab = "", main ="",
       axes = F,
       xlim = c(0.5,2.5), ylim = c(0.5,7))
  axis(1, at = 1:2, labels = c("control","ginger"), 
       lwd = 2, cex.axis = 1.1)
  axis(2, las = 2, lwd = 2, cex.axis = 1.1)
  mtext("condition", 1, 2.5, font= 2, cex = 1.2)
  mtext("average rating", 2, 3.5, font= 2, cex = 1.2)
  mtext(header, 3, 1, font= 2, cex = 1.3)
  grid(col = "darkgray")
  
  #aggregate by subject
  agg <- with(dat, aggregate(rating, list(Condition, subject), 
                    function(x) mean(x, na.rm = T)))
  
  cols <- c("darkgray", "lightgoldenrod")
  cols2 <- c(rgb(169, 169, 169, alpha = 100, maxColorValue = 255),
             rgb(238, 221, 130, alpha = 100, maxColorValue = 255))
  for (i in unique(dat$Condition)){
    # get points to plot
    pp <- agg[agg$Group.1==i,]$x
    
    # plot violin
    violinCustom(data = pp, xpos = i+1,
                 shade = cols2[i+1], scaling = viol_scale)
    
    # plot points
    points(pp ~ jitter(rep(i+1,length(pp)),
                       factor = 1, amount = 0.2),
           pch = ".", cex = 3, col = cols[i+1])
  }
  
}

```

```{r}
#plot means instead
plot_means(dat, "all data")

```

```{r}
#all conitions
for (cond in unique(dat$valence)){
  d_tmp <- subset(dat, valence == cond)
  plot_means(dat = d_tmp, header = cond)
}
```


#Preliminaries: Analysis of means

Since the authors use t-tests and Anovas in their original paper, we use multi-level models to test the hypothesis, ignoring for now the fact that ratings is ordinal. 

##Reproducing original analysis

```{r}
agg <- with(dat, 
            aggregate(rating, list(Condition,valence, subject),mean))
names(agg) <- c("condition", "valence", "subject", "rating")
```

*Result 1: ginger does not reduce digust for highly rated stimuli*

```{r}
agg_1 <- subset(agg, valence == "high")
t.test(rating ~ condition, data = agg_1)
```

R uses a Welch t-test instead of an independent samples, but it matches the reported results.

*Result 2: ginger reduces digust for medium rated stimuli*

```{r}
agg_1 <- subset(agg, valence == "medium")
t.test(rating ~ condition, data = agg_1)
```

This is *marginally* significant, but **passes** the .05 level and matches the reported results.

To be fair, the authors could have reported a one-sided test since the hypothesis is obviously directional:

```{r}
t.test(rating ~ condition, data = agg_1, alternative = "greater")
```


##Using a multi-level Gaussian model instead

Since the main results/effects are reported for the medium items, let's focus on that. The point here is to show that the Gaussian model is a bad idea. 

We let condition slopes vary both by subject and stimulus item, and set some weakly informative regularizing priors.

```{r}
dat$condition_c <- ifelse(dat$Condition==1,0.5,-0.5)
```


```{r brms medium data basic fit, results='hide', cache=T}
dat_fit <- subset(dat, valence == "medium")
dat_fit <- dat_fit[complete.cases(dat_fit),] # get rid of some (1) missing data
dat_fit$item <- factor(dat_fit$item)

pp <- c(set_prior("normal(0,.5)", "b"),
        set_prior("normal(5,5)", "Intercept"),
        set_prior("lkj(3)", "cor"),
        set_prior("normal(0,1)", "sd", group = "item"),
        set_prior("normal(0,.5)", "sd", group = "subject"),
        set_prior("normal(0,2)", "sigma"))

ff <- bf(rating ~ 1 + condition_c +
           (1 + condition_c | subject) + 
           (1 + condition_c | item))

fit_med <- brm(formula = ff, data = dat_fit, 
               family = gaussian(), prior = pp,
               sample_prior = T,
               cores = n_cores, chains = 6,
               iter = 2e3, warmup = 1e3,
               control = list(adapt_delta = 0.95))
```

```{r}
summary(fit_med)
```

```{r}
post <- posterior_samples(fit_med, "^b")
color_scheme_set("red")
mcmc_areas(post,
           prob = .8,
           prob_outer = .97)
```

We can see that the model does not seem to support the conclusion about an effect of ginger on ratings. However, we can question how reasonable it is with this gaussian assumption: 


```{r}
pp_check(fit_med)
```

So instead lets fit a model adapted to Likert scale data.

#Multi-level ordinal response model

##Selecting a model

*brms* supports several ordinal and category models. There is a recent [tutorial paper](https://osf.io/cu8jv/) available that details this a bit more. 

Here we will use a simple model called the cumulative model (or graded response model), assuming that there is a continuous  *latent* variable underlying participants responses. When the value of that underlying crosses certain thresholds this translates to the different ordinal responses observed. The model is cumulative since we can relate the thresholds to a cumulative distribution function, getting the probability of observing a response at least as high as that threshold. For this workshop we assume an underlying normal distributoin, leading to a *probit* model. Other choices are possible and the linked paper expands on this greatly. 

We can illustrate the probit and some hypothetical thresholds:

```{r}
plot(NA, xlim=c(-3,3), ylim = c(0,1),
      xlab = "threshold", ylab = "cumulative probability",
      main="", cex.axis = 1.2, cex.lab = 1.4)
cord.x <- c(-.7,seq(-.7,1,0.01),1)
cord.y <- c(0,pnorm(seq(-.7,1,0.01)),0)
polygon(cord.x,cord.y,col='steelblue', border = F)
abline(v=c(-.7,1), col = "darkblue", lty = 2, lwd=2)
curve(pnorm, from = -3, to = 3, add=T, lwd=3)
text(x = 0, y = 0.2, 
     labels = paste(round(pnorm(1)-pnorm(-.7),2)*100,"%"),
     col = "white", cex = 1.5)
```

When we estimate the model we will get **N-1** intercepts reflecting the thresholds for **N** categories. The thresholds are just deviates from a standard normal (z-scores). So to get the probability of a specific category we subtract the cumulative probability of one threshold from the one preceeding it. 

In the model we additionally formulate effects of variables we measured on the underlying latent variable, in forms of a regression equation. In this model it is assumed that that the effect is uniform over categories. Other ordinal models, however, allow for the estimation of category specific effects. 

After all that, the actual model formula is very simple:

```{r}
dat_fit$Condition_f <- factor(dat_fit$Condition, 
                              labels = c("control", "ginger"))

ff <- bf(rating ~ 1 + Condition_f +
           (1 + Condition_f | subject) + 
           (1 + Condition_f | item),
         family = cumulative("probit"))
```

##Prior predictive checks

When we are fitting something considerably more complicated, it is extra important to check our modeling assumptions and our priors!

```{r}
#what are our priors
kable(get_prior(formula = ff,
          data = dat_fit)) %>%
  kable_styling(c("striped", "condensed"), full_width = F)
```

There is some extra machinery to fit the model here, in that we specify a function to set initial values for the sampling process. To see how that connects with the underlying Stan code, see the Appendix.

```{r}
pp <- c(set_prior("normal(0,3)", class = "Intercept"),
        set_prior("normal(0,0.5)", class = "b"),
        set_prior("normal(0,1)", class = "sd", group = "item"),
        set_prior("normal(0,1)", class = "sd", group = "subject"),
        set_prior("lkj(3)", class = "cor"))

tmp_dat <- make_standata(ff,
                            data = dat_fit, 
                         prior = pp)

#a function of initial values
initfun <- function() {
  list(
    b = array(rnorm(tmp_dat$K, 0, 0.1), dim = length(tmp_dat$K)),
    temp_Intercept = seq(-2,2,length.out = tmp_dat$ncat-1) + rnorm(tmp_dat$ncat-1,0,0.1),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    L_1 = diag(tmp_dat$M_1)
  )
}
```

```{r prior likert 1, results='hide', cache=T}
mod_lik_pr <- brm(ff, 
               data = dat_fit, prior = pp,
               chains = 6, cores = n_cores,
               iter = 2e3, warmup = 1e3,
               inits = initfun,
               sample_prior = "only")
```



```{r}
marginal_effects(mod_lik_pr, categorical = T)
pp_check(mod_lik_pr)
```

Ok so this is not great. 

Below is what I ended up with after going back and forth and tweaking a bit. 

```{r prior likert 2, results='hide', cache=T}
pp <- c(set_prior("normal(0,1)", class = "Intercept"),
        set_prior("normal(0,0.5)", class = "b"),
        set_prior("normal(0,1)", class = "sd", group = "item"),
        set_prior("normal(0,1)", class = "sd", group = "subject"),
        set_prior("lkj(3)", class = "cor"))

mod_lik_pr1 <- brm(ff, 
               data = dat_fit, prior = pp,
               chains = 6, cores = n_cores,
               iter = 2e3, warmup = 1e3,
               inits = initfun,
               sample_prior = "only")
```

```{r}
marginal_effects(mod_lik_pr1, categorical = T)
pp_check(mod_lik_pr1)
```

That looks reasonable enough.

##Fitting the model

Now we fit the model, with some added control statements to help get rid of the warnings we saw earlier.

```{r first likert model, results='hide', cache=T}
mod_lik <- brm(ff, 
               data = dat_fit, prior = pp,
               chains = 6, cores = n_cores,
               iter = 2e3, warmup = 1e3,
               inits = initfun,
               sample_prior = T,
               control= list(adapt_delta =0.95,
                             max_treedepth = 15))
```

##Diagnostics

Again we want to run diagnostics on our model to make sure it is sensible.

```{r}
all_hats <- rhat(mod_lik)
mcmc_rhat_hist(all_hats)
```

```{r}
neffs <- neff_ratio(mod_lik)
mcmc_neff_hist(neffs, binwidth = 0.05)
```

```{r}
# which parameters have low neff
mcmc_neff(neffs[neffs<0.1]) + yaxis_text(hjust = 1)
```

We see that the model is sampling really inefficiently for some parameters. To fix this we need to adjust the priors. Again, here I went back and forth adjusting priors and refitting the model until I was satisfied. The final results is below:


```{r main likert model, results='hide', cache=T}
pp <- c(set_prior("normal(0,1)", class = "Intercept"),
        set_prior("normal(0,0.5)", class = "b"),
        set_prior("normal(0,1)", class = "sd", group = "item"),
        set_prior("normal(0,0.5)", class = "sd", group = "subject", coef = "Intercept"),
        set_prior("normal(0,0.15)", class = "sd", group = "subject", coef = "Condition_fginger"),
        set_prior("lkj(3)", class = "cor"))

mod_lik2 <- brm(ff, 
               data = dat_fit, prior = pp,
               chains = 6, cores = n_cores,
               iter = 4e3, warmup = 2e3,
               inits = initfun,
               sample_prior = T,
               control= list(adapt_delta =0.98,
                             max_treedepth = 15))
```

```{r}
neffs <- neff_ratio(mod_lik2)
mcmc_neff_hist(neffs, binwidth = 0.05)
```

And we're good!


##Posterior predictive checks

OK, let's evaluate the posterior of the model. 

```{r}
summary(mod_lik2)
```

```{r}
pp_check(mod_lik2)
```

This looks very good (compare to gaussian model earlier!).


```{r}
marginal_effects(mod_lik2, categorical = T)
```

On a first pass, this might seem reasonable, there looks like a small difference between categories and they vary in their expected probabilities. However, compare to the actual data:

```{r}
plot_prop(dat_fit, "the data")
```

We see that the modal response is 7 and not 4 as in that plot. But we also saw in the first posterior density check that the model seems to be predicting the data. What is going on here?

First we can verify that the model is predicting the data well:

```{r}
ypred_sum <- predict(mod_lik2)
str(ypred_sum)
```

```{r}
observed <- prop.table(table(dat_fit$rating))
predicted <- colMeans(ypred_sum)
cbind(observed, predicted)
```

The difference, it turns out that the *marginal_effects()* call uses the population-level estimates **only**, while *predict()* **also** uses the group-level estimates. Let's verify this by a quick calculation, and then think about why this matters in this model. 

*NOTE: We are usually much more interested in the predictions coming from the full model, so its in a sense much more important that they have good concordance with the data.  Part of this is an excuse to delve deeper into the nitty gritty of evaluating the model.*

```{r}
#extract the population level estimates
estimates <- fixef(mod_lik2)
estimates
```

```{r}
#keep the intercepts
estimates <- estimates[1:6]

#get cumulative probabilities
prob_est <- pnorm(estimates)

# add 1 for final category
prob_est <- c(prob_est,1)
prob_est
```
```{r}
#probailities of each response [for control condition]
c(prob_est[1], diff(prob_est))
```

We see that these match the expected values in the left panel of the marginal effects plot.

This is highly unusual. If we review the summary of the model, on thing that stands out is that we only have 3 levels of the item category. Most people doing multi-level modeling will probably tell you that is not ideal (but, importantly, its not a no-no either. 

Let's plot the data for each item separately:
```{r}
#items separately
for (it in unique(dat_fit$item)){
  d_tmp <- subset(dat, item == it)
  plot_prop(dat = d_tmp, header = it,
            y_max = 1)
}
```

Responses to the items appear to be really different. Let's look at the fitted coefficients:

```{r}
ranef(mod_lik2)$item
```

Two things stand out: 

1. There are **large** differences in the estimated intercept depending on item. (Remember, these are on a standard normal scale)  
2. There are very **small** differences in the estimated slopes - which is the variable we care about in this analysis.  

While not conclusive, it is hinting us that maybe we should look into this a bit more.

##Fitting an alternative model

Here we refit the model but drop the varying effects for items. 

```{r alt likert model, results='hide', cache=T}
ff <- bf(rating ~ 1 + Condition_f +
           (1 + Condition_f | subject),
         family = cumulative("probit"))

pp <- c(set_prior("normal(0,1)", class = "Intercept"),
        set_prior("normal(0,0.5)", class = "b"),
        set_prior("normal(0,0.5)", class = "sd", group = "subject", coef = "Intercept"),
        set_prior("normal(0,0.15)", class = "sd", group = "subject", coef = "Condition_fginger"),
        set_prior("lkj(3)", class = "cor"))

tmp_dat <- make_standata(ff,
                            data = dat_fit, 
                         prior = pp)

#a function of initial values
initfun <- function() {
  list(
    b = array(rnorm(tmp_dat$K, 0, 0.1), dim = length(tmp_dat$K)),
    temp_Intercept = seq(-2,2,length.out = tmp_dat$ncat-1) + rnorm(tmp_dat$ncat-1,0,0.1),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    L_1 = diag(tmp_dat$M_1)
  )
}

mod_lik3 <- brm(ff, 
               data = dat_fit, prior = pp,
               chains = 6, cores = n_cores,
               iter = 4e3, warmup = 2e3,
               inits = initfun,
               sample_prior = T,
               control= list(adapt_delta =0.98,
                             max_treedepth = 15))

```

```{r}
marginal_effects(mod_lik3, categorical = T)
```

The marginal effects now resemble our expectations better. 

```{r}
summary(mod_lik3)
```

We can see that the estimated intercepts are very different now as well. 

###Model comparison

Which model should we do our inference on. This is tricky and there won't be just one answer. Here we will rely on model comparison via LOO-CV. 

```{r likert loo, message =F, warning = F, cache = T}
loo_lik2 <- loo::loo(mod_lik2, cores = 3, reloo = T)
loo_lik3 <- loo::loo(mod_lik3, cores = 3, reloo = T)
```

```{r}
compare_ic(loo_lik2, loo_lik3)
```

We see that our original model is preferred after all!

Let's plot the category-wise predictions together with our data. 

```{r}
plot_prop(dat_fit, "data + model")

c_pred <- colMeans(ypred_sum[dat_fit$Condition==0,])
c_quant <- apply(ypred_sum[dat_fit$Condition==0,], 2, function(x) quantile(x, c(.25,.75)))
points(x = seq(1,7)-.2, y= c_pred, 
       pch = "X", cex = 1.8)
segments(x0 = seq(1,7)-.2, 
         y0 = c_quant[1,],
         y1 = c_quant[2,],
         lwd = 2)

g_pred <- colMeans(ypred_sum[dat_fit$Condition==1,])
g_quant <- apply(ypred_sum[dat_fit$Condition==1,], 2, function(x) quantile(x, c(.25,.75)))
points(x = seq(1,7)+.2, y= g_pred,
       pch = "X", cex = 1.8, col = "red")
segments(x0 = seq(1,7)+.2, 
         y0 = g_quant[1,],
         y1 = g_quant[2,],
         lwd = 2, col = "red")

```


##Inference

```{r}
post <- posterior_samples(mod_lik2, "^b")
q_97 <- quantile(post$b_Condition_fginger, c(0.015, 0.985))
q_97
```

We can also calculate the posterior probability that the effect is less than 0:

```{r}
prob <- sum(post$b_Condition_fginger<0)/length(post$b_Condition_fginger)
paste(prob*100,"% of samples are less than 0")
```

We can straightforwardly combine this information in a plot:

```{r}
mcmc_hist(post, "b_Condition_fginger", binwidth = 0.025) +
  geom_vline(xintercept = 0, size = 1.5, linetype = "dashed") +
  geom_segment(aes(x = q_97[1], xend = q_97[2], y = 5, yend = 5),
               size = 1.5, colour = "yellow") +
  annotate("text", 
           x = mean(q_97), y = 30,
           label = paste("97% CI: [", round(q_97[1],2), "," , round(q_97[2],2), "]" ),
           colour = "yellow", fontface = 2) +
  annotate("text",
           x = -0.9, y = 300,
           label = paste(prob*100,"% of samples \nare less than 0"),
           fontface = 2)
```

With this method the answer to our question is maybe!

Let's also assess with Bayes Factors.

```{r}
h <- hypothesis(mod_lik2, "Condition_fginger=0")
h
```
```{r}
plot(h)
```

With this method the answer is that it is as likely as it was before (see Appendix A for the effect of changing the prior)

#Summary

We have seen:

- It is probably a good idea to analyze ordinal data using ordinal models
- Finding a good model can take time
- Taking the multi-level structure of our data into account, affects what we conclude about the data using the model


#Appendix A : Priors and Bayes Factors


```{r alt likert model alt prior, results='hide', cache=T}
ff <- bf(rating ~ 1 + Condition_f +
           (1 + Condition_f | subject),
         family = cumulative("probit"))

pp <- c(set_prior("normal(0,1)", class = "Intercept"),
        set_prior("normal(0,0.1)", class = "b"),
        set_prior("normal(0,0.5)", class = "sd", group = "subject", coef = "Intercept"),
        set_prior("normal(0,0.15)", class = "sd", group = "subject", coef = "Condition_fginger"),
        set_prior("lkj(3)", class = "cor"))

tmp_dat <- make_standata(ff,
                            data = dat_fit, 
                         prior = pp)

#a function of initial values
initfun <- function() {
  list(
    b = array(rnorm(tmp_dat$K, 0, 0.1), dim = length(tmp_dat$K)),
    temp_Intercept = seq(-2,2,length.out = tmp_dat$ncat-1) + rnorm(tmp_dat$ncat-1,0,0.1),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    L_1 = diag(tmp_dat$M_1)
  )
}

mod_lik4 <- brm(ff, 
               data = dat_fit, prior = pp,
               chains = 6, cores = n_cores,
               iter = 4e3, warmup = 2e3,
               inits = initfun,
               sample_prior = T,
               control= list(adapt_delta =0.98,
                             max_treedepth = 15))

```



```{r}
post <- posterior_samples(mod_lik4, "^b")
q_97 <- quantile(post$b_Condition_fginger, c(0.015, 0.985))
q_97
```

Very similar to before.

```{r}
prob <- sum(post$b_Condition_fginger<0)/length(post$b_Condition_fginger)
paste(round(prob,2)*100,"% of samples are less than 0")
```

```{r}
mcmc_hist(post, "b_Condition_fginger", binwidth = 0.025) +
  geom_vline(xintercept = 0, size = 1.5, linetype = "dashed") +
  geom_segment(aes(x = q_97[1], xend = q_97[2], y = 5, yend = 5),
               size = 1.5, colour = "yellow") +
  annotate("text", 
           x = mean(q_97), y = 30,
           label = paste("97% CI: [", round(q_97[1],2), "," , round(q_97[2],2), "]" ),
           colour = "yellow", fontface = 2) +
  annotate("text",
           x = -0.9, y = 300,
           label = paste(prob*100,"% of samples \nare less than 0"),
           fontface = 2)
```

With this method the answer doesn't change much.

Let's also assess with Bayes Factors.

```{r}
h <- hypothesis(mod_lik4, "Condition_fginger=0")
h
```
```{r}
plot(h)
```

With this method the answer BF is suddenly close to 2. Not super-strong evidence, but a different story than earlier.


#Appendix B : Viewing Stan code

Viewing Stan code.

```{r}
ff <- bf(rating ~ 1 + Condition_f +
           (1 + Condition_f | subject) + 
           (1 + Condition_f | item),
         family = cumulative("probit"))

pp <- c(set_prior("normal(0,3)", class = "Intercept"),
        set_prior("normal(0,0.5)", class = "b"),
        set_prior("normal(0,1)", class = "sd", group = "item"),
        set_prior("normal(0,1)", class = "sd", group = "subject"),
        set_prior("lkj(3)", class = "cor"))

make_stancode(ff,data = dat_fit, prior = pp)
# parameters { 
#   vector[Kc] b;  // population-level effects 
#   ordered[ncat-1] temp_Intercept;  // temporary thresholds 
#   vector<lower=0>[M_1] sd_1;  // group-level standard deviations
#   matrix[M_1, N_1] z_1;  // unscaled group-level effects
#   // cholesky factor of correlation matrix
#   cholesky_factor_corr[M_1] L_1;
```

```{r}
tmp_dat <- make_standata(ff,
                            data = dat_fit, 
                         prior = pp)
str(tmp_dat, 1, give.attr = FALSE)
```


```{r}
initfun <- function() {
  list(
    b = array(rnorm(tmp_dat$K, 0, 0.1), dim = length(tmp_dat$K)),
    temp_Intercept = seq(-2,2,length.out = tmp_dat$ncat-1) + rnorm(tmp_dat$ncat-1,0,0.1),
    sd_1 = runif(tmp_dat$M_1, 0.5, 1),
    z_1 = matrix(rnorm(tmp_dat$M_1*tmp_dat$N_1, 0, 0.01),
                 tmp_dat$M_1, tmp_dat$N_1),
    L_1 = diag(tmp_dat$M_1)
  )
}
```

```{r}
#example of output
initfun()
```




