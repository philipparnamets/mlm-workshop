<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Philip" />


<title>mlm-workshop</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 52px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 57px;
  margin-top: -57px;
}

.section h2 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h3 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h4 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h5 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h6 {
  padding-top: 57px;
  margin-top: -57px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MLM-workshop</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Basic example</a>
</li>
<li>
  <a href="teting.html">Advanced example</a>
</li>
<li>
  <a href="further.html">Further reading</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">mlm-workshop</h1>
<h4 class="author"><em>Philip</em></h4>
<h4 class="date"><em>1 december 2018</em></h4>

</div>


<div id="load-data-and-packages" class="section level1">
<h1>Load data and packages</h1>
<pre class="r"><code>library(brms)
library(ggplot2)
library(bayesplot)
library(tidyverse)
library(kableExtra)
library(brmstools)
library(sjstats)</code></pre>
<pre class="r"><code>#set up number of cores to use for fitting
n_cores &lt;- parallel::detectCores()-1</code></pre>
<pre class="r"><code>d &lt;- read.delim(url(&quot;https://raw.githubusercontent.com/philipparnamets/mlm-workshop/master/data/d_export.txt&quot;))</code></pre>
<pre class="r"><code>summary(d)</code></pre>
<pre><code>##     Subject         Stimuli    Trustworthy     Face_trust        RT    
##  Min.   :13712   Min.   : 1   Min.   :-0.5   Min.   :2.8   Min.   : 0  
##  1st Qu.:14293   1st Qu.:16   1st Qu.:-0.5   1st Qu.:3.2   1st Qu.: 1  
##  Median :14767   Median :32   Median : 0.0   Median :3.4   Median : 1  
##  Mean   :15343   Mean   :32   Mean   : 0.0   Mean   :3.4   Mean   : 2  
##  3rd Qu.:16425   3rd Qu.:49   3rd Qu.: 0.5   3rd Qu.:3.6   3rd Qu.: 2  
##  Max.   :17668   Max.   :64   Max.   : 0.5   Max.   :4.2   Max.   :33</code></pre>
<p>Data comes from a trust learning experiment and represents a subset of 30 participants. Each participants did 30 trials with social partners from a trustworthy group and 30 trials with social partners from an untrustworthy group.</p>
<p>The aim of the analysis will be to understand if participant <strong>response times differ between groups</strong>.</p>
</div>
<div id="data-visualization-and-preparation" class="section level1">
<h1>Data visualization and preparation</h1>
<div id="relationship-between-rt-and-trustworthiness" class="section level2">
<h2>Relationship between RT and Trustworthiness</h2>
<pre class="r"><code># create factor
d$Trustworthy_fac &lt;- factor(d$Trustworthy, labels = c(&quot;Untrustworthy&quot;,
                                                      &quot;Trustworthy&quot;))

# summary table
d %&gt;% group_by(Trustworthy_fac) %&gt;%
  summarise(Average = mean(RT),
            Median = median(RT),
            SD = sd(RT),
            SE = SD/sqrt(n())) %&gt;%
  kable(.) %&gt;%
  kable_styling(., bootstrap_options = c(&quot;striped&quot;, &quot;condensed&quot;),
                full_width = F)</code></pre>
<table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Trustworthy_fac
</th>
<th style="text-align:right;">
Average
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
SD
</th>
<th style="text-align:right;">
SE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Untrustworthy
</td>
<td style="text-align:right;">
2.2
</td>
<td style="text-align:right;">
1.5
</td>
<td style="text-align:right;">
2.4
</td>
<td style="text-align:right;">
0.08
</td>
</tr>
<tr>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:right;">
1.8
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
1.8
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># plot
ggplot(data = d) +
  aes(x = Trustworthy_fac, y = RT, 
      fill = Trustworthy_fac) +
  geom_violin() +
  geom_boxplot(fill=&quot;gray&quot;, 
               notch = T ,
               width = 0.4) </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Lot’s of outliers, let’s filter the data at 10s, which seems to be plenty to make a response anyways..</p>
<pre class="r"><code># plot
d %&gt;% filter(RT&lt;10) %&gt;%
ggplot(data = .) +
  aes(x = Trustworthy_fac, y = RT, 
      fill = Trustworthy_fac) +
  geom_violin() +
  geom_boxplot(fill=&quot;gray&quot;, 
               notch = T ,
               width = 0.4) </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Response time data are skewed:</p>
<pre class="r"><code>ggplot(d) +
  aes(RT) +
  geom_histogram(binwidth = 0.25)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>One way of dealing with this is log-transforming the data.</p>
<pre class="r"><code># log transform
d$RT_log &lt;- log(d$RT)

#plot 
ggplot(d) +
  aes(RT_log) +
  geom_histogram(binwidth = 0.1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="face-trustworthiness" class="section level2">
<h2>Face trustworthiness</h2>
<p>The variable <em>Face_trust</em> captures how trustworthy the face of the partner has been rated to be.</p>
<pre class="r"><code>ggplot(d) +
  aes(Face_trust) +
  geom_histogram(binwidth = 0.1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code># scale variable
d$Face_trust_s &lt;- scale(d$Face_trust)</code></pre>
</div>
</div>
<div id="setting-up-the-model" class="section level1">
<h1>Setting up the model</h1>
<p>There are four important components to fitting a model in <em>brms</em>:</p>
<ul>
<li>Data</li>
<li>Likelihood</li>
<li>Model formula</li>
<li>Prior</li>
</ul>
<p>We have already set up our data.</p>
<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>SAY SOMETHING MORE ABOUT THE LIKELIHOOD HERE</p>
</div>
<div id="model-formula" class="section level2">
<h2>Model formula</h2>
<p><em>brms</em> uses the same syntax as <em>lme4</em>, which simplifies the transition.</p>
<pre class="r"><code>ff &lt;- 
  bf(RT ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli))</code></pre>
<p>We want to estimate the effect of Trustworthiness on response times, while taking into account the facial features of the social partners and their interaction wtih Trustworthiness. Additionally, we want to allow all slopes and intercept <strong>vary</strong> within participants and stimuli.</p>
</div>
<div id="priors" class="section level2">
<h2>Priors</h2>
<p>The arguably most tricky bit when it comes to getting started with Bayesian modelling is setting and understanding priors. First we see what priors the model requires:</p>
<pre class="r"><code>kable(get_prior(formula = ff, family = gaussian(),
          data = d)) %&gt;%
  kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = F)</code></pre>
<table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
prior
</th>
<th style="text-align:left;">
class
</th>
<th style="text-align:left;">
coef
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:left;">
resp
</th>
<th style="text-align:left;">
dpar
</th>
<th style="text-align:left;">
nlpar
</th>
<th style="text-align:left;">
bound
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
Face_trust_s
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
Trustworthy:Face_trust_s
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
lkj(1)
</td>
<td style="text-align:left;">
cor
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
cor
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
cor
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
student_t(3, 1, 10)
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
student_t(3, 0, 10)
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Face_trust_s
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy:Face_trust_s
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Face_trust_s
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy:Face_trust_s
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
student_t(3, 0, 10)
</td>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>We begin by setting some weakly informative priors.</p>
<pre class="r"><code>pp &lt;- c(set_prior(&quot;normal(0,1)&quot;, class = &quot;b&quot;), #this is for the population average slopes
        set_prior(&quot;normal(0,10)&quot;, class = &quot;sd&quot;, group = &quot;Subject&quot;),
        set_prior(&quot;normal(0,5)&quot;, class = &quot;sd&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;normal(0,5)&quot;, class = &quot;sigma&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Subject&quot;))</code></pre>
<pre class="r"><code>fit_rt &lt;- brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = &quot;only&quot;)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## Warning: There were 6 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre class="r"><code>rt1_prior &lt;- marginal_effects(fit_rt, re_formula = NULL,
                              method = &quot;predict&quot;)
plot(rt1_prior, ask = F)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-15-1.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-15-2.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-15-3.png" width="672" /></p>
<p>Let’s make some changes.</p>
<pre class="r"><code>pp &lt;- c(set_prior(&quot;normal(2,5)&quot;, class = &quot;Intercept&quot;),
        set_prior(&quot;normal(0,1)&quot;, class = &quot;b&quot;), #this is for the population average slopes
        set_prior(&quot;normal(0,1)&quot;, class = &quot;sd&quot;, group = &quot;Subject&quot;),
        set_prior(&quot;normal(0,1)&quot;, class = &quot;sd&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;normal(0,5)&quot;, class = &quot;sigma&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Subject&quot;))</code></pre>
<pre class="r"><code>fit_rt2 &lt;- brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = &quot;only&quot;)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## Warning: There were 4 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre class="r"><code>rt2_prior &lt;- marginal_effects(fit_rt2, re_formula = NULL,
                              method = &quot;predict&quot;)
plot(rt2_prior, ask = F)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-18-1.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-18-2.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-18-3.png" width="672" /></p>
<p>The model’s priors now covers a much more reasonable range of values.</p>
</div>
</div>
<div id="fitting-the-model" class="section level1">
<h1>Fitting the model</h1>
<p>Having gone through these steps, sampling from the model is straightforward:</p>
<pre class="r"><code>mod_rt2 &lt;-  brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = &quot;yes&quot;,
              control = list(adapt_delta = 0.8))</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<div id="model-diagnostics" class="section level2">
<h2>Model diagnostics</h2>
<p>Stan (and brms) will warn you if the model is having some particular problems converging. In particular it will tell you if there are <em>divergent transitions</em>, which are caused by the model not being able to explore the full parameter space. This can be tricky to fix, but models with divergent transitions should not be trusted. Changing the priors, and changing some of the sampler’s internal parameters can help - increasing adapt_delta over it’s default (0.8).</p>
<p>Other diagnostic quantities to pay attention to are the number of effective samples and <span class="math inline">\(\hat{R}\)</span> values.</p>
<p>Both are accessed from the model summary:</p>
<pre class="r"><code>summary(mod_rt2)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: RT ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) + (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli) 
##    Data: d (Number of observations: 1800) 
## Samples: 6 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 6000
## 
## Group-Level Effects: 
## ~Stimuli (Number of levels: 64) 
##                                            Estimate Est.Error l-95% CI
## sd(Intercept)                                  0.08      0.06     0.00
## sd(Trustworthy)                                0.17      0.13     0.01
## sd(Face_trust_s)                               0.10      0.06     0.01
## sd(Trustworthy:Face_trust_s)                   0.24      0.13     0.02
## cor(Intercept,Trustworthy)                    -0.05      0.33    -0.68
## cor(Intercept,Face_trust_s)                    0.05      0.33    -0.58
## cor(Trustworthy,Face_trust_s)                  0.07      0.33    -0.58
## cor(Intercept,Trustworthy:Face_trust_s)        0.07      0.34    -0.59
## cor(Trustworthy,Trustworthy:Face_trust_s)      0.01      0.33    -0.62
## cor(Face_trust_s,Trustworthy:Face_trust_s)     0.08      0.33    -0.56
##                                            u-95% CI Eff.Sample Rhat
## sd(Intercept)                                  0.21       3064 1.00
## sd(Trustworthy)                                0.46       2728 1.00
## sd(Face_trust_s)                               0.22       3138 1.00
## sd(Trustworthy:Face_trust_s)                   0.52       2811 1.00
## cor(Intercept,Trustworthy)                     0.60       7161 1.00
## cor(Intercept,Face_trust_s)                    0.65       6687 1.00
## cor(Trustworthy,Face_trust_s)                  0.67       6133 1.00
## cor(Intercept,Trustworthy:Face_trust_s)        0.67       6356 1.00
## cor(Trustworthy,Trustworthy:Face_trust_s)      0.64       6097 1.00
## cor(Face_trust_s,Trustworthy:Face_trust_s)     0.68       5707 1.00
## 
## ~Subject (Number of levels: 30) 
##                                            Estimate Est.Error l-95% CI
## sd(Intercept)                                  0.87      0.13     0.66
## sd(Trustworthy)                                0.53      0.14     0.28
## sd(Face_trust_s)                               0.22      0.08     0.06
## sd(Trustworthy:Face_trust_s)                   0.14      0.10     0.01
## cor(Intercept,Trustworthy)                    -0.40      0.19    -0.74
## cor(Intercept,Face_trust_s)                    0.02      0.23    -0.43
## cor(Trustworthy,Face_trust_s)                 -0.15      0.26    -0.62
## cor(Intercept,Trustworthy:Face_trust_s)        0.18      0.32    -0.49
## cor(Trustworthy,Trustworthy:Face_trust_s)     -0.03      0.32    -0.63
## cor(Face_trust_s,Trustworthy:Face_trust_s)    -0.02      0.32    -0.64
##                                            u-95% CI Eff.Sample Rhat
## sd(Intercept)                                  1.15       2062 1.00
## sd(Trustworthy)                                0.82       3090 1.00
## sd(Face_trust_s)                               0.38       1792 1.00
## sd(Trustworthy:Face_trust_s)                   0.38       4218 1.00
## cor(Intercept,Trustworthy)                     0.01       6135 1.00
## cor(Intercept,Face_trust_s)                    0.48       6900 1.00
## cor(Trustworthy,Face_trust_s)                  0.39       4569 1.00
## cor(Intercept,Trustworthy:Face_trust_s)        0.74       8730 1.00
## cor(Trustworthy,Trustworthy:Face_trust_s)      0.59       8743 1.00
## cor(Face_trust_s,Trustworthy:Face_trust_s)     0.60       9148 1.00
## 
## Population-Level Effects: 
##                          Estimate Est.Error l-95% CI u-95% CI Eff.Sample
## Intercept                    2.02      0.17     1.69     2.35       1465
## Trustworthy                 -0.36      0.14    -0.64    -0.08       4304
## Face_trust_s                 0.01      0.07    -0.13     0.13       6237
## Trustworthy:Face_trust_s     0.13      0.11    -0.09     0.35       7996
##                          Rhat
## Intercept                1.00
## Trustworthy              1.00
## Face_trust_s             1.00
## Trustworthy:Face_trust_s 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     1.90      0.03     1.84     1.97       9716 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>But can also be visualized using the Bayesplot package.</p>
<p>For <span class="math inline">\(\hat{R}\)</span> these should be approximately 1, and definitely no larger than 1.1.</p>
<pre class="r"><code>all_hats &lt;- rhat(mod_rt2)
color_scheme_set(&quot;red&quot;)
mcmc_rhat_hist(all_hats)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Effective sample size is a measure of how well the model is approximating the posterior distribution of interest. If there are very few effective samples, the resulting posterior is less reliable. This might be due to autocorrelation in the samples. Typically you can sample your model for longer to get more effective samples out of it.</p>
<p>Here we see that some parameters have a lower ratio of effective samples than others, but none are lower than 0.1, which is a <em>heuristic</em> value for when we should really worry:</p>
<pre class="r"><code>neffs &lt;- neff_ratio(mod_rt2)
mcmc_neff_hist(neffs)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>#here we can see which parameters were affected
mcmc_neff(neffs[neffs&lt;0.5]) + yaxis_text(hjust = 1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>If we had been worried about this, we might have wanted to plot the autocorrelation of, say, the intercept. We compare it with another parameter to see the difference:</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, c(&quot;b_Intercept&quot;, &quot;b_Trustworthy&quot;))
mcmc_acf(post[,1:2])</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><strong>&gt;&gt;&gt;If we had not been happy with the model diagnostics, we would refit the model before proceeding&lt;&lt;&lt;</strong></p>
</div>
</div>
<div id="working-with-the-posterior" class="section level1">
<h1>Working with the posterior</h1>
<p>The posterior samples reflect the outcome of the model fitting. We use this to further refine our understanding of the model and to hopefully learn something about the data that we are modelling.</p>
<div id="posterior-predictive-checks" class="section level2">
<h2>Posterior predictive checks</h2>
<p>The first thing is to see if the model is capturing relevant features of the data - in other words, if it plausibly is capturing the <em>underlying data generating process</em>.</p>
<pre class="r"><code>pp_check(mod_rt2)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Unurprisingly, our gaussian model cannot capture the full distribution of the data. (But we kind of knew that already, so let’s ignore it for now).</p>
<p>Another way of seeing this is how the minimum and maximum predictions differ from those in the data:</p>
<pre class="r"><code>#make a matrix of posterior predictions
y_pred &lt;- predict(mod_rt2, 
                  nsamples = 500, #500 samples per row of the original data frame
                  summary = F) #if =TRUE, then we will get the mean and 95% CIs instead
dim(y_pred)</code></pre>
<pre><code>## [1]  500 1800</code></pre>
<pre class="r"><code>#minimum value
ppc_stat(y = d$RT, yrep = y_pred,
         stat = &quot;min&quot;,
         binwidth = 0.05)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>This reinforces what we knew from the density overlay above.</p>
<pre class="r"><code>#maximum value
ppc_stat(y = d$RT, yrep = y_pred,
         stat = &quot;max&quot;,
         binwidth = 0.05)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>This reinforces what we know from the prior-predictive checks, the model will not predict extremely large values. In this case, we would probably consider the response time of 30s to be an outlier, and not be concerned about this at all..</p>
<p>We can also make posterior checks dependent on leves of a grouping variable. For example how good is the model at capturing the mean?</p>
<pre class="r"><code>ppc_stat_grouped(y = d$RT, 
                 yrep = y_pred, 
                 group = d$Trustworthy_fac,
                 binwidth = 0.025)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Just like when we looked at the prior predictive checks, we can get the marginal predictions:</p>
<pre class="r"><code>plot(marginal_effects(mod_rt2), ask = F)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-30-1.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-30-2.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-30-3.png" width="672" /></p>
<p>You could also make your own plot by using the <em>y_pred</em> values, since these are really the same thing. Here we show the full posterior distribution as a boxplot, the average RT in the data (black) and the average predictions (red dot).</p>
<pre class="r"><code>d$mu_pred &lt;- apply(y_pred,2,mean)

ggplot(d) + 
  aes(x=Trustworthy_fac, y = mu_pred) + 
  geom_boxplot(notch = T) +
  stat_summary(mapping = aes(y=RT, x = Trustworthy_fac), 
               fun.data = &quot;mean_se&quot;, geom = &quot;linerange&quot;, size = 2) + 
  stat_summary(fun.data = &quot;mean_se&quot;, colour = &quot;red&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
</div>
<div id="summarizing-the-model" class="section level2">
<h2>Summarizing the model</h2>
<p>We have already seen the full model summary. Let’s make some visualizations as well.</p>
<p>To do so we will extract samples from the model.</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^b&quot;) 
# ^b is regular expression to select all the population level parameters (beginning with &quot;b_&quot;))</code></pre>
<p>We can plot the coefficients:</p>
<pre class="r"><code>mcmc_intervals(post)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>We are not particularly interested in the intercept, given how the model was set up, so lets omit it:</p>
<pre class="r"><code>mcmc_areas(post[,-1],
           prob = 0.8,
           prob_outer = 0.95)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>Having fit a multi-level model we have access to lots of other parameters that we wouldn’t have had otherwise. Let’s a quick example. First, we extract the correlation estimates for the Subject level parameters.</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^cor_Subject&quot;) #correlation parameters
mcmc_areas(post,
           prob = 0.8,
           prob_outer = 0.95)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>We see that a fairly large negative correlation between subjects’ interecept and slopes of Trustworthiness. This would indicate that participants that are slower overall, also differentiate less between conditions in the speed of their responses. To see this, we can look at the estimates for the actual slopes for each participant.</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^r_Subject&quot;) # all Subject level parameters
dim(post)  #120 parameters, 30 subject * 4 estimates</code></pre>
<pre><code>## [1] 6000  120</code></pre>
<p>Here we plot the average slope, with lines indicating the uncertainty in the estimate (50% level)</p>
<pre class="r"><code>mu_slopes &lt;- apply(post, 2, mean)
mu_uncert &lt;- apply(post, 2, function(x) quantile(x, c(0.25,.75)))

plot(x = mu_slopes[1:30], y = mu_slopes[31:60],
     xlab = &quot;Intercept&quot;, ylab = &quot;Effect of Trustworthiness&quot;,
     pch = &quot;.&quot;, cex = 5)
abline(h=0, lty = 2)
grid(col = &quot;gray&quot;)
segments(x0 = mu_slopes[1:30],
         y0 = mu_uncert[1, 31:60],
         y1 = mu_uncert[2, 31:60],
         lwd = 1, col = &quot;darkgray&quot;)
segments(y0 = mu_slopes[31:60],
         x0 = mu_uncert[1, 1:30],
         x1 = mu_uncert[2, 1:30],
         lwd = 1, col = &quot;darkgray&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>Group varying estimates are deviations from the average, population level estimate. So to understand this plot in terms of the actual effect implied we need to add the population level.</p>
<pre class="r"><code>mu_slopes &lt;- apply(post, 2, mean)
mu_uncert &lt;- apply(post, 2, function(x) quantile(x, c(0.25,.75)))
trust_av &lt;- fixef(mod_rt2)[2,1]
inter_av &lt;- fixef(mod_rt2)[1,1]

plot(x = mu_slopes[1:30]+inter_av, 
     y = mu_slopes[31:60]+trust_av,
     xlab = &quot;Intercept&quot;, ylab = &quot;Effect of Trustworthiness&quot;,
     pch = &quot;.&quot;, cex = 5)
abline(h=0, lty = 2)
grid(col = &quot;gray&quot;)
segments(x0 = mu_slopes[1:30]+inter_av,
         y0 = mu_uncert[1, 31:60]+trust_av,
         y1 = mu_uncert[2, 31:60]+trust_av,
         lwd = 1, col = &quot;darkgray&quot;)
segments(y0 = mu_slopes[31:60]+trust_av,
         x0 = mu_uncert[1, 1:30]+inter_av,
         x1 = mu_uncert[2, 1:30]+inter_av,
         lwd = 1, col = &quot;darkgray&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>From this plot we also see that for example two participants have a positive effect of Trustworthiness. We can plot the group-level average slopes together with population level slope to get a sense for this variation in relation to the average effect. (note that this tool would be particularly useful if you are doing a “random effects” meta analysis)</p>
<pre class="r"><code>forest(mod_rt2, grouping = &quot;Subject&quot;, pars = &quot;Trustworthy&quot;)</code></pre>
<pre><code>## Picking joint bandwidth of 0.0553</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<div id="r2" class="section level3">
<h3>R2</h3>
<p>In some situations it can be interesting to calculate <span class="math inline">\(R^2\)</span>. The quantity computed is a <a href="https://github.com/jgabry/bayes_R2/blob/master/bayes_R2.pdf">Bayesian generalization</a> of the typical <span class="math inline">\(R^2\)</span>.</p>
<pre class="r"><code>bayes_R2(mod_rt2)</code></pre>
<pre><code>##    Estimate Est.Error Q2.5 Q97.5
## R2      0.2     0.017 0.17  0.23</code></pre>
<p>As with all things Bayesian, we not only get a point estimate but also a distribution.</p>
<pre class="r"><code># get full posterior 
r2 &lt;- data.frame(bayes_R2(mod_rt2, summary = F))

ggplot(data =r2) +
  aes(x = R2) +
  geom_density(fill = &quot;firebrick&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
</div>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<p>What conclusions can we draw from the model regarding the parameter estimates and the effect of Trustworthiness on response times?</p>
<div id="posterior-probabilities" class="section level3">
<h3>Posterior probabilities</h3>
<p>The most straightforward method is to assess the posterior distribution of the parameter of interest, in this case, the coefficient of trustworthiness.</p>
<p>For a typical two-sided inference, we can consider the posterior credible interval, for example the 97% interval, and see if this doesn’t include 0:</p>
<pre class="r"><code>q_97 &lt;- quantile(post$b_Trustworthy, c(0.015, 0.985))
q_97</code></pre>
<pre><code>## 1.5%  98% 
##   NA   NA</code></pre>
<p>We can also calculate the posterior probability that the effect is less than 0:</p>
<pre class="r"><code>prob &lt;- sum(post$b_Trustworthy&lt;0)/length(post$b_Trustworthy)
paste(prob*100,&quot;% of samples are less than 0&quot;)</code></pre>
<pre><code>## [1] &quot;NaN % of samples are less than 0&quot;</code></pre>
<p>We can straightforwardly combine this information in a plot:</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^b&quot;)
mcmc_hist(post, &quot;b_Trustworthy&quot;, binwidth = 0.025) +
  geom_vline(xintercept = 0, size = 1.5, linetype = &quot;dashed&quot;) +
  geom_segment(aes(x = q_97[1], xend = q_97[2], y = 5, yend = 5),
               size = 1.5, colour = &quot;blue&quot;) +
  annotate(&quot;text&quot;, 
           x = mean(q_97), y = 20,
           label = paste(&quot;97% CI: [&quot;, round(q_97[1],2), &quot;,&quot; , round(q_97[2],2), &quot;]&quot; ),
           colour = &quot;blue&quot;, fontface = 2) +
  annotate(&quot;text&quot;,
           x = -0.9, y = 300,
           label = paste(prob*100,&quot;% of samples \nare less than 0&quot;),
           fontface = 2)</code></pre>
<pre><code>## Warning: Removed 6000 rows containing missing values (geom_segment).</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_text).</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
</div>
<div id="hdi-rope" class="section level3">
<h3>HDI &amp; ROPE</h3>
<p>An alternative to quantile-based credible intervals is the HDI (Highest posterior Density Interval). The HDI is constructed to be the narrowest range containg X% of the posterior samples. In many cases these will be the same (symmetric posteriors centered on the mode/median). John Kruschke has written a lot about the HDI &amp; ROPE approach.</p>
<p>To get HDI intervals:</p>
<pre class="r"><code>hdi(mod_rt2, prob= c(.89, .97))</code></pre>
<pre><code>## 
## # Highest Density Interval
## 
##                                  HDI(89%)      HDI(97%)
##  b_Intercept                [ 1.75  2.29] [ 1.66  2.40]
##  b_Trustworthy              [-0.58 -0.13] [-0.66 -0.05]
##  b_Face_trust_s             [-0.10  0.12] [-0.14  0.15]
##  b_Trustworthy.Face_trust_s [-0.04  0.32] [-0.10  0.38]
##  sigma                      [ 1.85  1.96] [ 1.83  1.97]</code></pre>
<p>The sjstats package also allows us to get a summary with HDIs:</p>
<pre class="r"><code>tidy_stan(mod_rt2)</code></pre>
<pre><code>## 
## # Summary Statistics of Stan-Model
## 
##                           estimate std.error      HDI(89%) ratio rhat mcse
##  Intercept                    2.02      0.17 [ 1.75  2.29]  0.24    1    0
##  Trustworthy                 -0.35      0.14 [-0.58 -0.13]  0.72    1    0
##  Face_trust_s                 0.01      0.07 [-0.10  0.12]  1.04    1    0
##  Trustworthy.Face_trust_s     0.13      0.11 [-0.04  0.32]  1.33    1    0
##  sigma                        1.90      0.03 [ 1.85  1.96]  1.62    1    0</code></pre>
<p>We can use the HDIs just as the credible intervals in the previous section. But they come into their own when combined with ROPE-testing. (ROPE = Region of Practical Equivalence). The ROPE defines the ranges of parameter values that we consider to be equivalent to a null finding. We then compare the HDI to the ROPE to make a decision about the parameter. If the HDI excludes the parameter entirely, we reject the ROPE region. If the HDI is entirely within the ROPE, we accept the ROPE values. And otherwise we are undecidided. (<strong>BUT</strong> We can still make probabilistic statements about our parameter!)</p>
<p>Defining a ROPE is tricky, and will depend on knowledge about the effect of interest and the underlying theory. Here pre-registration can play a strong role.</p>
<p>For our example, let’s say we consider effects smaller than 75ms as meaningless and equivalent to 0. (Of course, there is nothing that says that you have to have the same ROPEs for all parameters, but if you scaled the variables you likely will..)</p>
<p>This tells us how much of the posterior is inside and outside the ROPE region.</p>
<pre class="r"><code>rope(mod_rt2, rope = c(-0.075,0.075))</code></pre>
<pre><code>## 
## # Proportions of samples inside and outside the ROPE
## 
##                             inside outside
##  b_Intercept                  0.0%  100.0%
##  b_Trustworthy                2.0%   98.0%
##  b_Face_trust_s              73.5%   26.5%
##  b_Trustworthy.Face_trust_s  26.2%   73.8%
##  sigma                        0.0%  100.0%</code></pre>
<p>To get the full equivalence test:</p>
<pre class="r"><code>equi_test(mod_rt2, rope = c(-.075,.075))</code></pre>
<pre><code>## 
## # Test for Practical Equivalence of Model Predictors
## 
##   Effect Size: 0.10
##          ROPE: [-0.07 0.07]
##       Samples: 6000
## 
##                                    H0 %inROPE      HDI(95%)
##  b_Intercept (*)               reject    0.00 [ 1.69  2.35]
##  b_Trustworthy                 reject    2.03 [-0.63 -0.08]
##  b_Face_trust_s             undecided   73.52 [-0.12  0.13]
##  b_Trustworthy.Face_trust_s undecided   26.18 [-0.10  0.34]
##  sigma                         reject    0.00 [ 1.84  1.97]</code></pre>
<pre><code>## 
## (*) the number of effective samples may be insufficient for some parameters</code></pre>
<p>Currently, these functions use 95% HDIs, but this hopefully will change in a future release.</p>
</div>
<div id="bayes-factors" class="section level3">
<h3>Bayes Factors</h3>
<p>Another method of inference is via Bayes Factors which quanitify the relative evidence in favor or against the null hypothesis. While it is possible to calculate Bayes Factors for entire models (see the <em>bridgesampling</em> (package)[<a href="https://cran.r-project.org/web/packages/bridgesampling/index.html" class="uri">https://cran.r-project.org/web/packages/bridgesampling/index.html</a>]), we will look at a simpler method at the parameter level. This is the Savage-Dickey ratio, which simply is the ratio between the prior density and the posterior density at the test value for your hypothesis (0 for a typical null hypothesis).</p>
<pre class="r"><code>h &lt;- hypothesis(mod_rt2, 
                c(&quot;Trustworthy=0&quot;,
                  &quot;Face_trust_s=0&quot;,
                  &quot;Trustworthy:Face_trust_s=0&quot;))
kable(h$hypothesis) %&gt;%
  kable_styling(c(&quot;condensed&quot;, &quot;striped&quot;), full_width = F)</code></pre>
<table class="table table-condensed table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Hypothesis
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Est.Error
</th>
<th style="text-align:right;">
CI.Lower
</th>
<th style="text-align:right;">
CI.Upper
</th>
<th style="text-align:right;">
Evid.Ratio
</th>
<th style="text-align:right;">
Post.Prob
</th>
<th style="text-align:left;">
Star
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Trustworthy) = 0
</td>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
-0.64
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.29
</td>
<td style="text-align:right;">
0.23
</td>
<td style="text-align:left;">
<ul>
<li></td>
</tr>
<tr>
<td style="text-align:left;">
(Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
-0.13
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
15.10
</td>
<td style="text-align:right;">
0.94
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
(Trustworthy:Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
4.33
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>By default, the Bayes Factor (called Evid.Ratio here) is reported as evidence <em>for</em> the null, the <span class="math inline">\(BF_{01}\)</span>. If we want the evidence against the null, <span class="math inline">\(BF_{10}\)</span>, we simply take the reciprocal.</p>
<pre class="r"><code>bf_10 &lt;- 1/h$hypothesis$Evid.Ratio
h_new &lt;- cbind(h$hypothesis, bf_10)
names(h_new) &lt;- c(names(h$hypothesis), &quot;BF_10&quot;)
kable(h_new) %&gt;%
  kable_styling(c(&quot;condensed&quot;, &quot;striped&quot;), full_width = F)</code></pre>
<table class="table table-condensed table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Hypothesis
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Est.Error
</th>
<th style="text-align:right;">
CI.Lower
</th>
<th style="text-align:right;">
CI.Upper
</th>
<th style="text-align:right;">
Evid.Ratio
</th>
<th style="text-align:right;">
Post.Prob
</th>
<th style="text-align:left;">
Star
</th>
<th style="text-align:right;">
BF_10
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Trustworthy) = 0
</td>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
-0.64
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.29
</td>
<td style="text-align:right;">
0.23
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:right;">
3.44
</td>
</tr>
<tr>
<td style="text-align:left;">
(Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
-0.13
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
15.10
</td>
<td style="text-align:right;">
0.94
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.07
</td>
</tr>
<tr>
<td style="text-align:left;">
(Trustworthy:Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.35
</td>
<td style="text-align:right;">
4.33
</td>
<td style="text-align:right;">
0.81
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.23
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>We can also visualize the hypothesis test:</p>
<pre class="r"><code>plot(h)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<p><strong>NOTE</strong>, Bayes Factors are <em>very</em> sensitive to the priors, and it is easy to BF-hack ‘marginal’ results. Pre-registration is useful here as well.</p>
</div>
<div id="model-comparison" class="section level3">
<h3>Model comparison</h3>
<p>In some cases we are not interested in making direct inferences about the parameter values but are interested in competing models. This is particularly the case if we are worried about overfitting - since more parameters will always improvde the fit of the model.</p>
<pre class="r"><code>mod_rt3 &lt;- brm(RT ~ 1 + Trustworthy + Face_trust_s  + 
     (1 + Trustworthy + Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s | Stimuli),
     prior = pp, family = gaussian(), data = d,
     iter = 2e3, warmup = 1e3,
     chains = 6, cores = n_cores)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<p>The current gold standard for this kind of model comparison is approximate leave-one-out cross validation (LOO). Note that this can be computationally taxing. An alternative, also supported by brms, is WAIC - (Widely Applicable Information Criterion).</p>
<pre class="r"><code>#only run if you have time to wait a bit..
loo2 &lt;- loo(mod_rt2, cores = n_cores, reloo = T)
loo3 &lt;- loo(mod_rt3, cores = n_cores, reloo = T)</code></pre>
<p>(For the purpose of this workshop we ignore the warning, but we really shouldn’t be..)</p>
<pre class="r"><code>compare_ic(loo2, loo3)</code></pre>
<pre><code>##                    LOOIC    SE
## mod_rt2           7580.6 371.6
## mod_rt3           7571.4 371.5
## mod_rt2 - mod_rt3    9.2   4.9</code></pre>
<p>There is no robust difference in predictive performance between the models.</p>
<p>There is a lot more to be said about LOO, and there is <a href="http://mc-stan.org/loo/articles/index.html">great online documentation for it</a>.</p>
</div>
</div>
</div>
<div id="refitting-the-model" class="section level1">
<h1>Refitting the model</h1>
<p>This is not a full analysis, but just a brief demonstration of how it could have looked if we had fitted the model to the log-transformed response times instead.</p>
<pre class="r"><code>ff &lt;- 
  bf(RT_log ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli))


pp &lt;- c(set_prior(&quot;normal(0.4, .2)&quot;, class = &quot;Intercept&quot;),
        set_prior(&quot;normal(0,.2)&quot;, class = &quot;b&quot;), #this is for the population average slopes
        set_prior(&quot;normal(0,.2)&quot;, class = &quot;sd&quot;, group = &quot;Subject&quot;),
        set_prior(&quot;normal(0,.2)&quot;, class = &quot;sd&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;normal(0,.5)&quot;, class = &quot;sigma&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Subject&quot;))


mod_rt_log &lt;- brm(formula = ff, data = d,
                  family = gaussian(), prior = pp,
                  iter = 2e3, warmup = 1e3, 
                  chains = 6, cores = n_cores)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code>pp_check(mod_rt_log)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-56-1.png" width="672" /></p>
<pre class="r"><code>#make a matrix of posterior predictions
y_pred &lt;- predict(mod_rt_log, 
                  nsamples = 500, #500 samples per row of the original data frame
                  summary = F) #if =TRUE, then we will get the mean and 95% CIs instead</code></pre>
<pre class="r"><code>#minimum value
ppc_stat(y = d$RT_log, yrep = y_pred,
         stat = &quot;min&quot;,
         binwidth = 0.025)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<pre class="r"><code>#maximum value
ppc_stat(y = d$RT_log, yrep = y_pred,
         stat = &quot;max&quot;,
         binwidth = 0.025)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<pre class="r"><code>ppc_stat_grouped(y = d$RT_log, 
                 yrep = y_pred, 
                 group = d$Trustworthy_fac,
                 binwidth = 0.005)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>We can see the the posterior predidictions are better than before. The downside is that now all our predictors are on a log scale:</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt_log, &quot;^b&quot;)
mcmc_areas(post,
           prob = 0.8,
           prob_outer = 0.95)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-61-1.png" width="672" /></p>
<p>Alternatively we would have changed the likelihood used in the model, using a distribution known to be very good with human response times like the ex-Gaussian (supported by <em>brms</em>). See <a href="http://www.tascl.org/uploads/4/9/3/3/49339445/4_.pdf">Heathcote, Popiel &amp; Mewhort, 1991</a> .</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
