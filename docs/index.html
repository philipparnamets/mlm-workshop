<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Philip" />


<title>mlm-workshop</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 52px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 57px;
  margin-top: -57px;
}

.section h2 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h3 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h4 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h5 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h6 {
  padding-top: 57px;
  margin-top: -57px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">MLM-workshop</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Basic example</a>
</li>
<li>
  <a href="teting.html">Advanced example</a>
</li>
<li>
  <a href="further.html">Further reading</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">mlm-workshop</h1>
<h4 class="author"><em>Philip</em></h4>
<h4 class="date"><em>1 december 2018</em></h4>

</div>


<div id="overview" class="section level1">
<h1>Overview</h1>
<p>The goal of the following is to serve as <em>breadth first</em> introduction to using <em>brms</em> to estimate multi-level regression models. By doing so, the hope is to sketch a complete Baysian workflow and touch on the many practical considerations one comes across when fitting statistical models. Even though the workshop is geared towards multi-level models, <em>brms</em> supports single level models and as the workflow used here applies also in that case.</p>
</div>
<div id="load-data-and-packages" class="section level1">
<h1>Load data and packages</h1>
<pre class="r"><code>library(brms)
library(ggplot2)
library(bayesplot)
library(tidyverse)
library(kableExtra)
library(brmstools)
library(sjstats)</code></pre>
<pre class="r"><code>#set up number of cores to use for fitting
n_cores &lt;- parallel::detectCores()-1</code></pre>
<pre class="r"><code>d &lt;- read.delim(url(&quot;https://raw.githubusercontent.com/philipparnamets/mlm-workshop/master/data/d_export.txt&quot;))</code></pre>
<pre class="r"><code>summary(d)</code></pre>
<pre><code>##     Subject         Stimuli    Trustworthy     Face_trust        RT    
##  Min.   :13712   Min.   : 1   Min.   :-0.5   Min.   :2.8   Min.   : 0  
##  1st Qu.:14293   1st Qu.:16   1st Qu.:-0.5   1st Qu.:3.2   1st Qu.: 1  
##  Median :14767   Median :32   Median : 0.0   Median :3.4   Median : 1  
##  Mean   :15343   Mean   :32   Mean   : 0.0   Mean   :3.4   Mean   : 2  
##  3rd Qu.:16425   3rd Qu.:49   3rd Qu.: 0.5   3rd Qu.:3.6   3rd Qu.: 2  
##  Max.   :17668   Max.   :64   Max.   : 0.5   Max.   :4.2   Max.   :33</code></pre>
<p>We will be working with real dataset. The data comes from a trust learning experiment and represents a subset of 30 participants. Each participants did 30 trials with social partners from a trustworthy group and 30 trials with social partners from an untrustworthy group.</p>
<p>The aim of the analysis will be to understand if participant <strong>response times differ between groups</strong>.</p>
</div>
<div id="data-visualization-and-preparation" class="section level1">
<h1>Data visualization and preparation</h1>
<p>The first step of any data analysis is exploratory data analysis - plotting and understanding your variables. This is not only important of itself, but also to understand how the variables are distributed and to ensure that the dataset is in a format that is suitable for statistical modeling.</p>
<div id="relationship-between-rt-and-trustworthiness" class="section level2">
<h2>Relationship between RT and Trustworthiness</h2>
<pre class="r"><code># create factor
d$Trustworthy_fac &lt;- factor(d$Trustworthy, labels = c(&quot;Untrustworthy&quot;,
                                                      &quot;Trustworthy&quot;))

# summary table
d %&gt;% group_by(Trustworthy_fac) %&gt;%
  summarise(Average = mean(RT),
            Median = median(RT),
            SD = sd(RT),
            SE = SD/sqrt(n())) %&gt;%
  kable(.) %&gt;%
  kable_styling(., bootstrap_options = c(&quot;striped&quot;, &quot;condensed&quot;),
                full_width = F)</code></pre>
<table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Trustworthy_fac
</th>
<th style="text-align:right;">
Average
</th>
<th style="text-align:right;">
Median
</th>
<th style="text-align:right;">
SD
</th>
<th style="text-align:right;">
SE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Untrustworthy
</td>
<td style="text-align:right;">
2.2
</td>
<td style="text-align:right;">
1.5
</td>
<td style="text-align:right;">
2.4
</td>
<td style="text-align:right;">
0.08
</td>
</tr>
<tr>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:right;">
1.8
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
1.8
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
</tbody>
</table>
<pre class="r"><code># plot
ggplot(data = d) +
  aes(x = Trustworthy_fac, y = RT, 
      fill = Trustworthy_fac) +
  geom_violin() +
  geom_boxplot(fill=&quot;gray&quot;, 
               notch = T ,
               width = 0.4) </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Lot’s of outliers, let’s filter the data at 10s, just to make the plot more clear.</p>
<pre class="r"><code># plot
d %&gt;% filter(RT&lt;10) %&gt;%
ggplot(data = .) +
  aes(x = Trustworthy_fac, y = RT, 
      fill = Trustworthy_fac) +
  geom_violin() +
  geom_boxplot(fill=&quot;gray&quot;, 
               notch = T ,
               width = 0.4) </code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>So we can see that the data appears to support there being a difference in response times between the groups.</p>
<p>Response time data are skewed and the following histogram makes that clear:</p>
<pre class="r"><code>ggplot(d) +
  aes(RT) +
  geom_histogram(binwidth = 0.25)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>One way of dealing with this is log-transforming the data. In what follows we will use the untransformed data though, but see the Appendix for a model with this tranformation applied.</p>
<pre class="r"><code># log transform
d$RT_log &lt;- log(d$RT)

#plot 
ggplot(d) +
  aes(RT_log) +
  geom_histogram(binwidth = 0.1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="face-trustworthiness" class="section level2">
<h2>Face trustworthiness</h2>
<p>The variable <em>Face_trust</em> captures how trustworthy the face of the partner has been rated to be.</p>
<pre class="r"><code>ggplot(d) +
  aes(Face_trust) +
  geom_histogram(binwidth = 0.1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>We want to scale the variable so it roughly has the same dimensions as our Trustworthiness variable, which I precoded to have the levels -0.5 and 0.5. Since the dataset is balanced, this means that both variables will effectively be means-centered, meaning the intercept of the regression model will have a natural interpretation.</p>
<pre class="r"><code># scale variable
d$Face_trust_s &lt;- scale(d$Face_trust)</code></pre>
</div>
</div>
<div id="setting-up-the-model" class="section level1">
<h1>Setting up the model</h1>
<p>There are three important components to fitting a model in <em>brms</em>:</p>
<ul>
<li>Data</li>
<li>Likelihood &amp; model specification</li>
<li>Prior</li>
</ul>
<p>We have already set up our data.</p>
<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>In Bayesian statistics we think of the statistical model as a candidate <em>generating process</em> for the data we observed. In the case of the type of statistical models most commonly used - i.e. ANOVA’s or standard linear regressions - the underlying assumption is that the data were generated by adraws from a Gaussian (normal) distribution. This will also be our assumption in what follows. This is the <em>likelihood</em> function for our model. The likelihood is just as much part of the statisitcal model as the data, the parameters and the priors.</p>
<p><span class="math display">\[ \text{Response time} \sim \text{Normal}(\mu, \sigma) \]</span></p>
<p>We could have just as well chosen another likelihood if pragmatic (i.e. the data) or theoretical considerations motivated us to do so. <em>brms</em> supports a wide range of data generating processes, and additionally the option to custom code one in directly in Stan. To see the options available, you can type:</p>
<pre class="r"><code>help(&quot;brmsfamily&quot;)</code></pre>
<p>The next step is to relate the parameters of the likelihood function to variables in the dataset. We assume that the variables we are interested affect the mean, <span class="math inline">\(\mu\)</span>, of the distribution and not its variance, <span class="math inline">\(\sigma^2\)</span>. (Stan and thus <em>brms</em> parametrizes the normal distribution with its standard deviation, <span class="math inline">\(\sigma\)</span>)</p>
<p>The model we want to fit in words: We want to estimate the effect of Trustworthiness on response times, while taking into account the facial features of the social partners and their interaction wtih Trustworthiness. Additionally, we want to allow all slopes and intercept <strong>vary</strong> within participants and stimuli. This latter statement summaries the multi-level aspect of the model.</p>
<p>We can translate this into the formula syntax used by <em>brms</em>, but also into notation. Depending on what you are used to seeing, either may be informative for you.</p>
<div id="model-formula-in-brms" class="section level3">
<h3>Model formula in brms</h3>
<p><em>brms</em> uses the same syntax as <em>lme4</em>, which simplifies the transition.</p>
<pre class="r"><code>ff &lt;- 
  bf(RT ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli))</code></pre>
<p>We could optionally also define the likelihood already here rather than in the call to <em>brms</em> later.</p>
<pre class="r"><code>ff &lt;- 
  bf(RT ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli),
     family = gaussian())</code></pre>
</div>
<div id="model-formula-in-notation" class="section level3">
<h3>Model formula in notation</h3>
<p><span class="math display">\[  y_i \sim \text{Normal}(\mu_i, \sigma) \]</span></p>
<p><span class="math display">\[ \mu_i \sim (\beta_0 + \beta_{0,subj[i]} + \beta_{0,stim[i]}) + 
(\beta_1 + \beta_{1,subj[i]} + \beta_{1,stim[i]})trust_i +  
(\beta_2 + \beta_{2,subj[i]} + \beta_{2,stim[i]})face_i +
(\beta_3 + \beta_{3,subj[i]} + \beta_{3,stim[i]})tXf_i \]</span></p>
<p><span class="math display">\[ \begin{bmatrix}
\beta_{[0,subj]} \\
\beta_{[1,subj]} \\
\beta_{[2,subj]} \\
\beta_{[3,subj]} \\
\end{bmatrix}
\sim \text{MVNormal}(
\begin{pmatrix}
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}, S_{subj}
\end{pmatrix} \]</span></p>
<p><span class="math display">\[\begin{bmatrix}
\beta_{[0,stim]} \\
\beta_{[1,stim]} \\
\beta_{[2,stim]} \\
\beta_{[3,stim]} \\
\end{bmatrix}
\sim \text{MVNormal}(
\begin{pmatrix}
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}, S_{stim}
\end{pmatrix}  \]</span></p>
<p>This is alot to take in, but let’s quickly break it down by line:</p>
<ol style="list-style-type: decimal">
<li>This is just our likelihood.<br />
</li>
<li>This is the regression relating the mean to our variables. There are a series of statements consisting of three parameters (<span class="math inline">\(\beta s\)</span>) added together. The ones without additional subscripts are the <em>population level-effects</em>. In other words what we would fit if this was a single-level model. (Also known as <em>fixed effects</em>), The <span class="math inline">\(\beta s\)</span> with additiona subscripts are the <em>varying interecepts and slopes</em>. (Also known as <em>random effects</em>). Each subject/stimulus gets its own deviation from the average effect.<br />
</li>
<li>Is the likelihood for the by-subject varying interecepts and slopes. We assume that the effects are drawn from a multivariate normal distribution, with mean 0 and covariance matrix S (more on this later).<br />
</li>
<li>Does the same as 3, but for the subjects.</li>
</ol>
<p>It is totally ok not to be comfortable with this if this is the first time you see the model laid out like this. Importantly, this is the <strong>same</strong> formulation as if you had been using a frequentist modeling approach and specified the formula in <em>lme4</em> instead!</p>
</div>
</div>
<div id="priors" class="section level2">
<h2>Priors</h2>
<p>The arguably most tricky bit when it comes to getting started with Bayesian modeling is setting and understanding priors.</p>
<p>Goals with priors:</p>
<ul>
<li>Capture explicit knowledge</li>
<li>Regularize the parameters</li>
<li>Identify your model</li>
<li>Bayes factor optimal</li>
</ul>
<p>We will mainly focus on regularizing (“weakly informative”) priors. Both as this helps the model to not get too excited about extreme data, and since in psychology we typically do not have the kind of stable quantiative information that would make explicit priors reasonable, (Although expections: Stroop effects, paramerter distributions in some cognitive models, standardized tests).</p>
<p>First we see what priors the model requires:</p>
<pre class="r"><code>kable(get_prior(formula = ff, family = gaussian(),
          data = d)) %&gt;%
  kable_styling(c(&quot;striped&quot;, &quot;condensed&quot;), full_width = F)</code></pre>
<table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
prior
</th>
<th style="text-align:left;">
class
</th>
<th style="text-align:left;">
coef
</th>
<th style="text-align:left;">
group
</th>
<th style="text-align:left;">
resp
</th>
<th style="text-align:left;">
dpar
</th>
<th style="text-align:left;">
nlpar
</th>
<th style="text-align:left;">
bound
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
Face_trust_s
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
b
</td>
<td style="text-align:left;">
Trustworthy:Face_trust_s
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
lkj(1)
</td>
<td style="text-align:left;">
cor
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
cor
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
cor
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
student_t(3, 1, 10)
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
student_t(3, 0, 10)
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Face_trust_s
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy:Face_trust_s
</td>
<td style="text-align:left;">
Stimuli
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Face_trust_s
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Intercept
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
sd
</td>
<td style="text-align:left;">
Trustworthy:Face_trust_s
</td>
<td style="text-align:left;">
Subject
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
student_t(3, 0, 10)
</td>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<p>We begin by setting some priors.</p>
<pre class="r"><code>pp &lt;- c(set_prior(&quot;normal(0,1)&quot;, class = &quot;b&quot;), #this is for the population average slopes
        set_prior(&quot;normal(0,10)&quot;, class = &quot;sd&quot;, group = &quot;Subject&quot;), #std dev of varying effects
        set_prior(&quot;normal(0,5)&quot;, class = &quot;sd&quot;, group = &quot;Stimuli&quot;), #std dev of varying effects
        set_prior(&quot;normal(0,5)&quot;, class = &quot;sigma&quot;), # this is for the std dev of the likelihood (&quot;residual&quot;)
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Stimuli&quot;), #correlation of varying effects
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Subject&quot;)) #correlation of varying effects</code></pre>
<p>A good way of understanding what the priors imply is to plot them, for example:</p>
<pre class="r"><code>plot(density(rnorm(1e6, 0, 1)),
     main = &quot;normal(0,1) prior&quot;,
     xlab =&quot;&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>The <em>LKJ(n)</em> prior is a special prior for correlation matrices, where n=1 implies a flat distribution and n&lt;1 more weight for extreme correlations and n&gt;1 less weight for extreme correlations.</p>
<p>Nevertheless, it can be difficult to know what the combined implications of the priors is for what response times values the model will think are likely. For this, prior predictive checks are essential.</p>
<div id="prior-predictive-checks" class="section level3">
<h3>Prior predictive checks</h3>
<p>The idea with prior predictive checks is to sample from the model, without taking the data into account. This will generate samples which reflect the priors and the likelihood only.</p>
<pre class="r"><code>fit_rt &lt;- brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = &quot;only&quot;)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code>#this plots the posterior density against the actual density
pp_check(fit_rt)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<pre class="r"><code>#this plots the marginal predictions for each parameter and interaction
rt1_prior &lt;- marginal_effects(fit_rt, re_formula = NULL,
                              method = &quot;predict&quot;)
plot(rt1_prior, ask = F)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-18-1.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-18-2.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-18-3.png" width="672" /></p>
<p>We can see the model generates values way more extreme than anything we observed. By tighteneing the priors the model will both sample more efficiently and be better at regularizing its estimates.</p>
<pre class="r"><code>pp &lt;- c(set_prior(&quot;normal(2,5)&quot;, class = &quot;Intercept&quot;),
        set_prior(&quot;normal(0,1)&quot;, class = &quot;b&quot;), #this is for the population average slopes
        set_prior(&quot;normal(0,1)&quot;, class = &quot;sd&quot;, group = &quot;Subject&quot;),
        set_prior(&quot;normal(0,1)&quot;, class = &quot;sd&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;normal(0,5)&quot;, class = &quot;sigma&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Subject&quot;))</code></pre>
<pre class="r"><code>fit_rt2 &lt;- brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = &quot;only&quot;)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code>pp_check(fit_rt2)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>rt2_prior &lt;- marginal_effects(fit_rt2, re_formula = NULL,
                              method = &quot;predict&quot;)
plot(rt2_prior, ask = F)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-21-1.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-21-2.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-21-3.png" width="672" /></p>
<p>The model’s priors now covers a much more reasonable range of values.</p>
</div>
<div id="the-full-model-in-notation" class="section level3">
<h3>The full model in notation</h3>
<p>Let’s add our priors to the model specification:</p>
<p><span class="math display">\[  y_i \sim \text{Normal}(\mu_i, \sigma) \]</span></p>
<p><span class="math display">\[ \mu_i \sim (\beta_0 + \beta_{0,subj[i]} + \beta_{0,stim[i]}) + 
(\beta_1 + \beta_{1,subj[i]} + \beta_{1,stim[i]})trust_i +  
(\beta_2 + \beta_{2,subj[i]} + \beta_{2,stim[i]})face_i +
(\beta_3 + \beta_{3,subj[i]} + \beta_{3,stim[i]})tXf_i \]</span></p>
<p><span class="math display">\[ \begin{bmatrix}
\beta_{[0,subj]} \\
\beta_{[1,subj]} \\
\beta_{[2,subj]} \\
\beta_{[3,subj]} \\
\end{bmatrix}
\sim \text{MVNormal}(
\begin{pmatrix}
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}, S_{subj}
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ \begin{bmatrix}
\beta_{[0,stim]} \\
\beta_{[1,stim]} \\
\beta_{[2,stim]} \\
\beta_{[3,stim]} \\
\end{bmatrix}
\sim \text{MVNormal}(
\begin{pmatrix}
\begin{bmatrix}
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}, S_{stim}
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ \beta_0 \sim \text{Normal}(2,5) \\
(\beta_{1},\beta_{2},\beta_{3}) \sim  \text{Normal}(0,1) \\
\sigma \sim \text{Normal}(0,5) \]</span></p>
<p><span class="math display">\[ S_{subj} = 
\begin{pmatrix}
\sigma_{0,subj} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \sigma_{1,subj} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sigma_{2,subj} &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_{3,subj} \\
\end{pmatrix} 
R_{subj}
\begin{pmatrix}
\sigma_{0,subj} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \sigma_{1,subj} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sigma_{2,subj} &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_{3,subj} \\
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ (\sigma_{0,subj},\sigma_{1,subj},\sigma_{2,subj},\sigma_{3,subj}) \sim \text{Normal}(0,1) \]</span></p>
<p><span class="math display">\[ R_{subj} \sim LKJ(3) \]</span></p>
<p><span class="math display">\[ S_{stim} = 
\begin{pmatrix}
\sigma_{0,stim} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \sigma_{1,stim} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sigma_{2,stim} &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_{3,stim} \\
\end{pmatrix} 
R_{stim}
\begin{pmatrix}
\sigma_{0,stim} &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; \sigma_{1,stim} &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sigma_{2,stim} &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma_{3,stim} \\
\end{pmatrix} \]</span></p>
<p><span class="math display">\[ (\sigma_{0,stim},\sigma_{1,stim},\sigma_{2,stim},\sigma_{3,stim}) \sim \text{Normal}(0,1) \]</span></p>
<p><span class="math display">\[ R_{stim} \sim LKJ(3) \]</span></p>
</div>
</div>
</div>
<div id="fitting-the-model" class="section level1">
<h1>Fitting the model</h1>
<p>Having gone through these steps, sampling from the model is now super-straightforward:</p>
<pre class="r"><code>mod_rt2 &lt;-  brm(formula = ff, 
              data = d, family = gaussian(),
              prior = pp,
              chains = 6, cores = 3,
              iter = 2e3, warmup = 1e3,
              sample_prior = &quot;yes&quot;,
              control = list(adapt_delta = 0.8))</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<div id="model-diagnostics" class="section level2">
<h2>Model diagnostics</h2>
<p>Stan (and brms) will warn you if the model is having some particular problems converging. In particular it will tell you if there are <em>divergent transitions</em>, which are caused by the model not being able to explore the full parameter space. This can be tricky to fix, but models with divergent transitions should not be trusted. Changing the priors, and changing some of the sampler’s internal parameters can help - increasing adapt_delta over it’s default (0.8).</p>
<p>Other diagnostic quantities to pay attention to are the number of effective samples and <span class="math inline">\(\hat{R}\)</span> values.</p>
<p>Both are accessed from the model summary:</p>
<pre class="r"><code>summary(mod_rt2)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: RT ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) + (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli) 
##    Data: d (Number of observations: 1800) 
## Samples: 6 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 6000
## 
## Group-Level Effects: 
## ~Stimuli (Number of levels: 64) 
##                                            Estimate Est.Error l-95% CI
## sd(Intercept)                                  0.08      0.06     0.00
## sd(Trustworthy)                                0.18      0.12     0.01
## sd(Face_trust_s)                               0.09      0.06     0.00
## sd(Trustworthy:Face_trust_s)                   0.24      0.13     0.02
## cor(Intercept,Trustworthy)                    -0.06      0.33    -0.67
## cor(Intercept,Face_trust_s)                    0.04      0.33    -0.60
## cor(Trustworthy,Face_trust_s)                  0.07      0.33    -0.59
## cor(Intercept,Trustworthy:Face_trust_s)        0.07      0.33    -0.56
## cor(Trustworthy,Trustworthy:Face_trust_s)      0.01      0.33    -0.63
## cor(Face_trust_s,Trustworthy:Face_trust_s)     0.09      0.32    -0.56
##                                            u-95% CI Eff.Sample Rhat
## sd(Intercept)                                  0.21       3382 1.00
## sd(Trustworthy)                                0.45       2777 1.00
## sd(Face_trust_s)                               0.23       3295 1.00
## sd(Trustworthy:Face_trust_s)                   0.51       2665 1.00
## cor(Intercept,Trustworthy)                     0.59       7176 1.00
## cor(Intercept,Face_trust_s)                    0.67       7657 1.00
## cor(Trustworthy,Face_trust_s)                  0.67       6303 1.00
## cor(Intercept,Trustworthy:Face_trust_s)        0.68       5560 1.00
## cor(Trustworthy,Trustworthy:Face_trust_s)      0.63       6221 1.00
## cor(Face_trust_s,Trustworthy:Face_trust_s)     0.67       5574 1.00
## 
## ~Subject (Number of levels: 30) 
##                                            Estimate Est.Error l-95% CI
## sd(Intercept)                                  0.87      0.12     0.67
## sd(Trustworthy)                                0.53      0.14     0.27
## sd(Face_trust_s)                               0.22      0.08     0.06
## sd(Trustworthy:Face_trust_s)                   0.14      0.10     0.01
## cor(Intercept,Trustworthy)                    -0.40      0.20    -0.75
## cor(Intercept,Face_trust_s)                    0.02      0.24    -0.45
## cor(Trustworthy,Face_trust_s)                 -0.14      0.26    -0.61
## cor(Intercept,Trustworthy:Face_trust_s)        0.19      0.32    -0.47
## cor(Trustworthy,Trustworthy:Face_trust_s)     -0.02      0.31    -0.61
## cor(Face_trust_s,Trustworthy:Face_trust_s)    -0.03      0.33    -0.63
##                                            u-95% CI Eff.Sample Rhat
## sd(Intercept)                                  1.15       1863 1.00
## sd(Trustworthy)                                0.83       2730 1.00
## sd(Face_trust_s)                               0.38       1497 1.00
## sd(Trustworthy:Face_trust_s)                   0.38       4134 1.00
## cor(Intercept,Trustworthy)                     0.02       6196 1.00
## cor(Intercept,Face_trust_s)                    0.48       8062 1.00
## cor(Trustworthy,Face_trust_s)                  0.38       4354 1.00
## cor(Intercept,Trustworthy:Face_trust_s)        0.74      10213 1.00
## cor(Trustworthy,Trustworthy:Face_trust_s)      0.58      10964 1.00
## cor(Face_trust_s,Trustworthy:Face_trust_s)     0.61       8749 1.00
## 
## Population-Level Effects: 
##                          Estimate Est.Error l-95% CI u-95% CI Eff.Sample
## Intercept                    2.03      0.16     1.71     2.36       1241
## Trustworthy                 -0.36      0.14    -0.64    -0.08       4877
## Face_trust_s                 0.01      0.07    -0.12     0.14       6644
## Trustworthy:Face_trust_s     0.13      0.11    -0.09     0.36       9084
##                          Rhat
## Intercept                1.00
## Trustworthy              1.00
## Face_trust_s             1.00
## Trustworthy:Face_trust_s 1.00
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
## sigma     1.90      0.03     1.84     1.97       9218 1.00
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>But can also be visualized using the Bayesplot package.</p>
<p>For <span class="math inline">\(\hat{R}\)</span> these should be approximately 1, and definitely no larger than 1.1.</p>
<pre class="r"><code>all_hats &lt;- rhat(mod_rt2)
color_scheme_set(&quot;red&quot;)
mcmc_rhat_hist(all_hats)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>Effective sample size is a measure of how well the model is approximating the posterior distribution of interest. If there are very few effective samples, the resulting posterior is less reliable. This might be due to autocorrelation in the samples. Typically you can sample your model for longer to get more effective samples out of it.</p>
<p>Here we see that some parameters have a lower ratio of effective samples than others, but none are lower than 0.1, which is a <em>heuristic</em> value for when we should really worry:</p>
<pre class="r"><code>neffs &lt;- neff_ratio(mod_rt2)
mcmc_neff_hist(neffs)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre class="r"><code>#here we can see which parameters were affected
mcmc_neff(neffs[neffs&lt;0.5]) + yaxis_text(hjust = 1)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>If we had been worried about this, we might have wanted to plot the autocorrelation of, say, the intercept. We compare it with another parameter to see the difference:</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, c(&quot;b_Intercept&quot;, &quot;b_Trustworthy&quot;))
mcmc_acf(post[,1:2])</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p><strong>&gt;&gt;&gt;If we had not been happy with the model diagnostics, we would refit the model before proceeding&lt;&lt;&lt;</strong></p>
</div>
</div>
<div id="working-with-the-posterior" class="section level1">
<h1>Working with the posterior</h1>
<p>The posterior samples reflect the outcome of the model fitting. We use this to further refine our understanding of the model and to hopefully learn something about the data that we are modelling.</p>
<div id="posterior-predictive-checks" class="section level2">
<h2>Posterior predictive checks</h2>
<p>The first thing is to see if the model is capturing relevant features of the data - in other words, if it plausibly is capturing the <em>underlying data generating process</em>.</p>
<pre class="r"><code>pp_check(mod_rt2)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Unurprisingly, our gaussian model cannot capture the full distribution of the data. (But we kind of knew that already, so let’s ignore it for now).</p>
<p>Another way of seeing this is how the minimum and maximum predictions differ from those in the data:</p>
<pre class="r"><code>#make a matrix of posterior predictions
y_pred &lt;- predict(mod_rt2, 
                  nsamples = 500, #500 samples per row of the original data frame
                  summary = F) #if =TRUE, then we will get the mean and 95% CIs instead
dim(y_pred)</code></pre>
<pre><code>## [1]  500 1800</code></pre>
<pre class="r"><code>#minimum value
ppc_stat(y = d$RT, yrep = y_pred,
         stat = &quot;min&quot;,
         binwidth = 0.05)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>This reinforces what we knew from the density overlay above.</p>
<pre class="r"><code>#maximum value
ppc_stat(y = d$RT, yrep = y_pred,
         stat = &quot;max&quot;,
         binwidth = 0.05)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>This reinforces what we know from the prior-predictive checks, the model will not predict extremely large values. In this case, we would probably consider the response time of 30s to be an outlier, and not be concerned about this at all..</p>
<p>We can also make posterior checks dependent on leves of a grouping variable. For example how good is the model at capturing the mean?</p>
<pre class="r"><code>ppc_stat_grouped(y = d$RT, 
                 yrep = y_pred, 
                 group = d$Trustworthy_fac,
                 binwidth = 0.025)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>Just like when we looked at the prior predictive checks, we can get the marginal predictions:</p>
<pre class="r"><code>plot(marginal_effects(mod_rt2), ask = F)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-32-1.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-32-2.png" width="672" /><img src="index_files/figure-html/unnamed-chunk-32-3.png" width="672" /></p>
<p>You could also make your own plot by using the <em>y_pred</em> values, since these are really the same thing. Here we show the full posterior distribution as a boxplot, the average RT in the data (black) and the average predictions (red dot).</p>
<pre class="r"><code>d$mu_pred &lt;- apply(y_pred,2,mean)

ggplot(d) + 
  aes(x=Trustworthy_fac, y = mu_pred) + 
  geom_boxplot(notch = T) +
  stat_summary(mapping = aes(y=RT, x = Trustworthy_fac), 
               fun.data = &quot;mean_se&quot;, geom = &quot;linerange&quot;, size = 2) + 
  stat_summary(fun.data = &quot;mean_se&quot;, colour = &quot;red&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
<div id="summarizing-the-model" class="section level2">
<h2>Summarizing the model</h2>
<p>We have already seen the full model summary. Let’s make some visualizations as well.</p>
<p>To do so we will extract samples from the model.</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^b&quot;) 
# ^b is regular expression to select all the population level parameters (beginning with &quot;b_&quot;))</code></pre>
<p>We can plot the coefficients:</p>
<pre class="r"><code>mcmc_intervals(post)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>We are not particularly interested in the intercept, given how the model was set up, so lets omit it:</p>
<pre class="r"><code>mcmc_areas(post[,-1],
           prob = 0.8,
           prob_outer = 0.95)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>Having fit a multi-level model we have access to lots of other parameters that we wouldn’t have had otherwise. Let’s a quick example. First, we extract the correlation estimates for the Subject level parameters.</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^cor_Subject&quot;) #correlation parameters
mcmc_areas(post,
           prob = 0.8,
           prob_outer = 0.95)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>We see that a fairly large negative correlation between subjects’ interecept and slopes of Trustworthiness. This would indicate that participants that are slower overall, also differentiate less between conditions in the speed of their responses. To see this, we can look at the estimates for the actual slopes for each participant.</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^r_Subject&quot;) # all Subject level parameters
dim(post)  #120 parameters, 30 subject * 4 estimates</code></pre>
<pre><code>## [1] 6000  120</code></pre>
<p>Here we plot the average slope, with lines indicating the uncertainty in the estimate (50% level)</p>
<pre class="r"><code>mu_slopes &lt;- apply(post, 2, mean)
mu_uncert &lt;- apply(post, 2, function(x) quantile(x, c(0.25,.75)))

plot(x = mu_slopes[1:30], y = mu_slopes[31:60],
     xlab = &quot;Intercept&quot;, ylab = &quot;Effect of Trustworthiness&quot;,
     pch = &quot;.&quot;, cex = 5)
abline(h=0, lty = 2)
grid(col = &quot;gray&quot;)
segments(x0 = mu_slopes[1:30],
         y0 = mu_uncert[1, 31:60],
         y1 = mu_uncert[2, 31:60],
         lwd = 1, col = &quot;darkgray&quot;)
segments(y0 = mu_slopes[31:60],
         x0 = mu_uncert[1, 1:30],
         x1 = mu_uncert[2, 1:30],
         lwd = 1, col = &quot;darkgray&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Group varying estimates are deviations from the average, population level estimate. So to understand this plot in terms of the actual effect implied we need to add the population level.</p>
<pre class="r"><code>mu_slopes &lt;- apply(post, 2, mean)
mu_uncert &lt;- apply(post, 2, function(x) quantile(x, c(0.25,.75)))
trust_av &lt;- fixef(mod_rt2)[2,1]
inter_av &lt;- fixef(mod_rt2)[1,1]

plot(x = mu_slopes[1:30]+inter_av, 
     y = mu_slopes[31:60]+trust_av,
     xlab = &quot;Intercept&quot;, ylab = &quot;Effect of Trustworthiness&quot;,
     pch = &quot;.&quot;, cex = 5)
abline(h=0, lty = 2)
grid(col = &quot;gray&quot;)
segments(x0 = mu_slopes[1:30]+inter_av,
         y0 = mu_uncert[1, 31:60]+trust_av,
         y1 = mu_uncert[2, 31:60]+trust_av,
         lwd = 1, col = &quot;darkgray&quot;)
segments(y0 = mu_slopes[31:60]+trust_av,
         x0 = mu_uncert[1, 1:30]+inter_av,
         x1 = mu_uncert[2, 1:30]+inter_av,
         lwd = 1, col = &quot;darkgray&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>From this plot we also see that for example two participants have a positive effect of Trustworthiness. We can plot the group-level average slopes together with population level slope to get a sense for this variation in relation to the average effect. (note that this tool would be particularly useful if you are doing a “random effects” meta analysis)</p>
<pre class="r"><code>forest(mod_rt2, grouping = &quot;Subject&quot;, pars = &quot;Trustworthy&quot;)</code></pre>
<pre><code>## Picking joint bandwidth of 0.0555</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<div id="r2" class="section level3">
<h3>R2</h3>
<p>In some situations it can be interesting to calculate <span class="math inline">\(R^2\)</span>. The quantity computed is a <a href="https://github.com/jgabry/bayes_R2/blob/master/bayes_R2.pdf">Bayesian generalization</a> of the typical <span class="math inline">\(R^2\)</span>.</p>
<pre class="r"><code>bayes_R2(mod_rt2)</code></pre>
<pre><code>##    Estimate Est.Error Q2.5 Q97.5
## R2      0.2     0.017 0.17  0.23</code></pre>
<p>As with all things Bayesian, we not only get a point estimate but also a distribution.</p>
<pre class="r"><code># get full posterior 
r2 &lt;- data.frame(bayes_R2(mod_rt2, summary = F))

ggplot(data =r2) +
  aes(x = R2) +
  geom_density(fill = &quot;firebrick&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-43-1.png" width="672" /></p>
</div>
</div>
<div id="inference" class="section level2">
<h2>Inference</h2>
<p>What conclusions can we draw from the model regarding the parameter estimates and the effect of Trustworthiness on response times?</p>
<div id="posterior-probabilities" class="section level3">
<h3>Posterior probabilities</h3>
<p>The most straightforward method is to assess the posterior distribution of the parameter of interest, in this case, the coefficient of trustworthiness.</p>
<p>For a typical two-sided inference, we can consider the posterior credible interval, for example the 97% interval, and see if this doesn’t include 0:</p>
<pre class="r"><code>q_97 &lt;- quantile(post$b_Trustworthy, c(0.015, 0.985))
q_97</code></pre>
<pre><code>## 1.5%  98% 
##   NA   NA</code></pre>
<p>We can also calculate the posterior probability that the effect is less than 0:</p>
<pre class="r"><code>prob &lt;- sum(post$b_Trustworthy&lt;0)/length(post$b_Trustworthy)
paste(prob*100,&quot;% of samples are less than 0&quot;)</code></pre>
<pre><code>## [1] &quot;NaN % of samples are less than 0&quot;</code></pre>
<p>We can straightforwardly combine this information in a plot:</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt2, &quot;^b&quot;)
mcmc_hist(post, &quot;b_Trustworthy&quot;, binwidth = 0.025) +
  geom_vline(xintercept = 0, size = 1.5, linetype = &quot;dashed&quot;) +
  geom_segment(aes(x = q_97[1], xend = q_97[2], y = 5, yend = 5),
               size = 1.5, colour = &quot;blue&quot;) +
  annotate(&quot;text&quot;, 
           x = mean(q_97), y = 20,
           label = paste(&quot;97% CI: [&quot;, round(q_97[1],2), &quot;,&quot; , round(q_97[2],2), &quot;]&quot; ),
           colour = &quot;blue&quot;, fontface = 2) +
  annotate(&quot;text&quot;,
           x = -0.9, y = 300,
           label = paste(prob*100,&quot;% of samples \nare less than 0&quot;),
           fontface = 2)</code></pre>
<pre><code>## Warning: Removed 6000 rows containing missing values (geom_segment).</code></pre>
<pre><code>## Warning: Removed 1 rows containing missing values (geom_text).</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
</div>
<div id="hdi-rope" class="section level3">
<h3>HDI &amp; ROPE</h3>
<p>An alternative to quantile-based credible intervals is the HDI (Highest posterior Density Interval). The HDI is constructed to be the narrowest range containg X% of the posterior samples. In many cases these will be the same (symmetric posteriors centered on the mode/median). John Kruschke has written a lot about the HDI &amp; ROPE approach.</p>
<p>To get HDI intervals:</p>
<pre class="r"><code>hdi(mod_rt2, prob= c(.89, .97))</code></pre>
<pre><code>## 
## # Highest Density Interval
## 
##                                  HDI(89%)      HDI(97%)
##  b_Intercept                [ 1.77  2.30] [ 1.67  2.40]
##  b_Trustworthy              [-0.58 -0.13] [-0.67 -0.04]
##  b_Face_trust_s             [-0.10  0.11] [-0.14  0.16]
##  b_Trustworthy.Face_trust_s [-0.05  0.31] [-0.11  0.38]
##  sigma                      [ 1.85  1.95] [ 1.83  1.97]</code></pre>
<p>The sjstats package also allows us to get a summary with HDIs:</p>
<pre class="r"><code>tidy_stan(mod_rt2)</code></pre>
<pre><code>## 
## # Summary Statistics of Stan-Model
## 
##                           estimate std.error      HDI(89%) ratio rhat mcse
##  Intercept                    2.03      0.16 [ 1.77  2.30]  0.21    1    0
##  Trustworthy                 -0.36      0.13 [-0.58 -0.13]  0.81    1    0
##  Face_trust_s                 0.01      0.06 [-0.10  0.11]  1.11    1    0
##  Trustworthy.Face_trust_s     0.13      0.11 [-0.05  0.31]  1.51    1    0
##  sigma                        1.90      0.03 [ 1.85  1.95]  1.54    1    0</code></pre>
<p>We can use the HDIs just as the credible intervals in the previous section. But they come into their own when combined with ROPE-testing. (ROPE = Region of Practical Equivalence). The ROPE defines the ranges of parameter values that we consider to be equivalent to a null finding. We then compare the HDI to the ROPE to make a decision about the parameter. If the HDI excludes the parameter entirely, we reject the ROPE region. If the HDI is entirely within the ROPE, we accept the ROPE values. And otherwise we are undecidided. (<strong>BUT</strong> We can still make probabilistic statements about our parameter!)</p>
<p>Defining a ROPE is tricky, and will depend on knowledge about the effect of interest and the underlying theory. Here pre-registration can play a strong role.</p>
<p>For our example, let’s say we consider effects smaller than 75ms as meaningless and equivalent to 0. (Of course, there is nothing that says that you have to have the same ROPEs for all parameters, but if you scaled the variables you likely will..)</p>
<p>This tells us how much of the posterior is inside and outside the ROPE region.</p>
<pre class="r"><code>rope(mod_rt2, rope = c(-0.075,0.075))</code></pre>
<pre><code>## 
## # Proportions of samples inside and outside the ROPE
## 
##                             inside outside
##  b_Intercept                  0.0%  100.0%
##  b_Trustworthy                2.3%   97.7%
##  b_Face_trust_s              73.4%   26.6%
##  b_Trustworthy.Face_trust_s  27.1%   72.9%
##  sigma                        0.0%  100.0%</code></pre>
<p>To get the full equivalence test:</p>
<pre class="r"><code>equi_test(mod_rt2, rope = c(-.075,.075))</code></pre>
<pre><code>## 
## # Test for Practical Equivalence of Model Predictors
## 
##   Effect Size: 0.10
##          ROPE: [-0.07 0.07]
##       Samples: 6000
## 
##                                    H0 %inROPE      HDI(95%)
##  b_Intercept (*)               reject    0.00 [ 1.71  2.36]
##  b_Trustworthy                 reject    2.30 [-0.64 -0.08]
##  b_Face_trust_s             undecided   73.38 [-0.12  0.14]
##  b_Trustworthy.Face_trust_s undecided   27.10 [-0.10  0.35]
##  sigma                         reject    0.00 [ 1.84  1.97]</code></pre>
<pre><code>## 
## (*) the number of effective samples may be insufficient for some parameters</code></pre>
<p>Currently, these functions use 95% HDIs, but this hopefully will change in a future release.</p>
</div>
<div id="bayes-factors" class="section level3">
<h3>Bayes Factors</h3>
<p>Another method of inference is via Bayes Factors which quanitify the relative evidence in favor or against the null hypothesis. While it is possible to calculate Bayes Factors for entire models (see the <em>bridgesampling</em> <a href="https://cran.r-project.org/web/packages/bridgesampling/index.html">package</a> ) , we will look at a simpler method at the parameter level. This is the Savage-Dickey ratio, which simply is the ratio between the prior density and the posterior density at the test value for your hypothesis (0 for a typical null hypothesis).</p>
<pre class="r"><code>h &lt;- hypothesis(mod_rt2, 
                c(&quot;Trustworthy=0&quot;,
                  &quot;Face_trust_s=0&quot;,
                  &quot;Trustworthy:Face_trust_s=0&quot;))
kable(h$hypothesis) %&gt;%
  kable_styling(c(&quot;condensed&quot;, &quot;striped&quot;), full_width = F)</code></pre>
<table class="table table-condensed table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Hypothesis
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Est.Error
</th>
<th style="text-align:right;">
CI.Lower
</th>
<th style="text-align:right;">
CI.Upper
</th>
<th style="text-align:right;">
Evid.Ratio
</th>
<th style="text-align:right;">
Post.Prob
</th>
<th style="text-align:left;">
Star
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Trustworthy) = 0
</td>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
-0.64
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:left;">
<ul>
<li></td>
</tr>
<tr>
<td style="text-align:left;">
(Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
-0.12
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
16.59
</td>
<td style="text-align:right;">
0.94
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
(Trustworthy:Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.36
</td>
<td style="text-align:right;">
4.74
</td>
<td style="text-align:right;">
0.83
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>By default, the Bayes Factor (called Evid.Ratio here) is reported as evidence <em>for</em> the null, the <span class="math inline">\(BF_{01}\)</span>. If we want the evidence against the null, <span class="math inline">\(BF_{10}\)</span>, we simply take the reciprocal.</p>
<pre class="r"><code>bf_10 &lt;- 1/h$hypothesis$Evid.Ratio
h_new &lt;- cbind(h$hypothesis, bf_10)
names(h_new) &lt;- c(names(h$hypothesis), &quot;BF_10&quot;)
kable(h_new) %&gt;%
  kable_styling(c(&quot;condensed&quot;, &quot;striped&quot;), full_width = F)</code></pre>
<table class="table table-condensed table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Hypothesis
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Est.Error
</th>
<th style="text-align:right;">
CI.Lower
</th>
<th style="text-align:right;">
CI.Upper
</th>
<th style="text-align:right;">
Evid.Ratio
</th>
<th style="text-align:right;">
Post.Prob
</th>
<th style="text-align:left;">
Star
</th>
<th style="text-align:right;">
BF_10
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
(Trustworthy) = 0
</td>
<td style="text-align:right;">
-0.36
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
-0.64
</td>
<td style="text-align:right;">
-0.08
</td>
<td style="text-align:right;">
0.37
</td>
<td style="text-align:right;">
0.27
</td>
<td style="text-align:left;">
<ul>
<li></td>
<td style="text-align:right;">
2.68
</td>
</tr>
<tr>
<td style="text-align:left;">
(Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.01
</td>
<td style="text-align:right;">
0.07
</td>
<td style="text-align:right;">
-0.12
</td>
<td style="text-align:right;">
0.14
</td>
<td style="text-align:right;">
16.59
</td>
<td style="text-align:right;">
0.94
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.06
</td>
</tr>
<tr>
<td style="text-align:left;">
(Trustworthy:Face_trust_s) = 0
</td>
<td style="text-align:right;">
0.13
</td>
<td style="text-align:right;">
0.11
</td>
<td style="text-align:right;">
-0.09
</td>
<td style="text-align:right;">
0.36
</td>
<td style="text-align:right;">
4.74
</td>
<td style="text-align:right;">
0.83
</td>
<td style="text-align:left;">
</td>
<td style="text-align:right;">
0.21
</td>
</tr>
</tbody>
</table></li>
</ul>
<p>We can also visualize the hypothesis test:</p>
<pre class="r"><code>plot(h)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p><strong>NOTE</strong>, Bayes Factors are <em>very</em> sensitive to the priors, and it is easy to BF-hack ‘marginal’ results. Pre-registration is useful here as well.</p>
</div>
<div id="model-comparison" class="section level3">
<h3>Model comparison</h3>
<p>In some cases we are not interested in making direct inferences about the parameter values but are interested in competing models. This is particularly the case if we are worried about overfitting - since more parameters will always improvde the fit of the model.</p>
<pre class="r"><code>mod_rt3 &lt;- brm(RT ~ 1 + Trustworthy + Face_trust_s  + 
     (1 + Trustworthy + Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s | Stimuli),
     prior = pp, family = gaussian(), data = d,
     iter = 2e3, warmup = 1e3,
     chains = 6, cores = n_cores)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<p>The current gold standard for this kind of model comparison is approximate leave-one-out cross validation (LOO). Note that this can be computationally taxing. An alternative, also supported by brms, is WAIC - (Widely Applicable Information Criterion).</p>
<pre class="r"><code>#only run if you have time to wait a bit..
loo2 &lt;- loo(mod_rt2, cores = n_cores, reloo = T)
loo3 &lt;- loo(mod_rt3, cores = n_cores, reloo = T)</code></pre>
<p>(For the purpose of this workshop we ignore the warning, but we really shouldn’t be..)</p>
<pre class="r"><code>compare_ic(loo2, loo3)</code></pre>
<pre><code>##                     LOOIC    SE
## mod_rt2           7570.42 365.7
## mod_rt3           7569.60 369.5
## mod_rt2 - mod_rt3    0.81   6.8</code></pre>
<p>There is no robust difference in predictive performance between the models.</p>
<p>There is a lot more to be said about LOO, and there is <a href="http://mc-stan.org/loo/articles/index.html">great online documentation for it</a>.</p>
</div>
</div>
</div>
<div id="appendix-refitting-the-model" class="section level1">
<h1>Appendix: Refitting the model</h1>
<p>This is not a full analysis, but just a brief demonstration of how it could have looked if we had fitted the model to the log-transformed response times instead.</p>
<pre class="r"><code>ff &lt;- 
  bf(RT_log ~ 1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s + 
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Subject) +
     (1 + Trustworthy + Face_trust_s + Trustworthy:Face_trust_s | Stimuli))


pp &lt;- c(set_prior(&quot;normal(0.4, .2)&quot;, class = &quot;Intercept&quot;),
        set_prior(&quot;normal(0,.2)&quot;, class = &quot;b&quot;), #this is for the population average slopes
        set_prior(&quot;normal(0,.2)&quot;, class = &quot;sd&quot;, group = &quot;Subject&quot;),
        set_prior(&quot;normal(0,.2)&quot;, class = &quot;sd&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;normal(0,.5)&quot;, class = &quot;sigma&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Stimuli&quot;),
        set_prior(&quot;lkj(3)&quot;, class = &quot;cor&quot;, group = &quot;Subject&quot;))


mod_rt_log &lt;- brm(formula = ff, data = d,
                  family = gaussian(), prior = pp,
                  iter = 2e3, warmup = 1e3, 
                  chains = 6, cores = n_cores)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code>pp_check(mod_rt_log)</code></pre>
<pre><code>## Using 10 posterior samples for ppc type &#39;dens_overlay&#39; by default.</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<pre class="r"><code>#make a matrix of posterior predictions
y_pred &lt;- predict(mod_rt_log, 
                  nsamples = 500, #500 samples per row of the original data frame
                  summary = F) #if =TRUE, then we will get the mean and 95% CIs instead</code></pre>
<pre class="r"><code>#minimum value
ppc_stat(y = d$RT_log, yrep = y_pred,
         stat = &quot;min&quot;,
         binwidth = 0.025)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-57-1.png" width="672" /></p>
<pre class="r"><code>#maximum value
ppc_stat(y = d$RT_log, yrep = y_pred,
         stat = &quot;max&quot;,
         binwidth = 0.025)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-58-1.png" width="672" /></p>
<pre class="r"><code>ppc_stat_grouped(y = d$RT_log, 
                 yrep = y_pred, 
                 group = d$Trustworthy_fac,
                 binwidth = 0.005)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<p>We can see the the posterior predidictions are better than before. The downside is that now all our predictors are on a log scale:</p>
<pre class="r"><code>post &lt;- posterior_samples(mod_rt_log, &quot;^b&quot;)
mcmc_areas(post,
           prob = 0.8,
           prob_outer = 0.95)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-60-1.png" width="672" /></p>
<p>Alternatively we would have changed the likelihood used in the model, using a distribution known to be very good with human response times like the ex-Gaussian (supported by <em>brms</em>). See <a href="http://www.tascl.org/uploads/4/9/3/3/49339445/4_.pdf">Heathcote, Popiel &amp; Mewhort, 1991</a> .</p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
